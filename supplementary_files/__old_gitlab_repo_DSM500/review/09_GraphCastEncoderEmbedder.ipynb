{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphCastEncoderEmbedder\n",
    "\n",
    "First used in [1] and defined in [2].\n",
    "\n",
    "```python\n",
    "\"\"\"GraphCast feature embedder for gird node features, multimesh node features,\n",
    "    grid2mesh edge features, and multimesh edge features.\"\"\"\n",
    "```\n",
    "\n",
    "- [1] https://vscode.dev/github/NVIDIA/modulus/blob/main/modulus/models/graphcast/graph_cast_net.py#L411\n",
    "- [2] https://vscode.dev/github/NVIDIA/modulus/blob/main/modulus/models/gnn_layers/embedder.py#L25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1230 10:17:31.080101708 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "%run review/__common.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoked as:\n",
    "# self.encoder_embedder = GraphCastEncoderEmbedder(\n",
    "#     input_dim_grid_nodes=input_dim_grid_nodes, # 31\n",
    "#     input_dim_mesh_nodes=input_dim_mesh_nodes, # 3 (dafault)\n",
    "#     input_dim_edges=input_dim_edges, # 4 (default)\n",
    "#     output_dim=hidden_dim, # 64\n",
    "#     hidden_dim=hidden_dim, # 64\n",
    "#     hidden_layers=hidden_layers, # 1 (default)\n",
    "#     activation_fn=activation_fn, # get_activation(\"silu\")\n",
    "#     norm_type=norm_type, # TELayerNorm\n",
    "#     recompute_activation=recompute_activation, # True\n",
    "# )\n",
    "\n",
    "# Defined as:\n",
    "# class GraphCastEncoderEmbedder(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim_grid_nodes: int = 474,\n",
    "#         input_dim_mesh_nodes: int = 3,\n",
    "#         input_dim_edges: int = 4,\n",
    "#         output_dim: int = 512,\n",
    "#         hidden_dim: int = 512,\n",
    "#         hidden_layers: int = 1,\n",
    "#         activation_fn: nn.Module = nn.SiLU(),\n",
    "#         norm_type: str = \"LayerNorm\",\n",
    "#         recompute_activation: bool = False,\n",
    "#     ):\n",
    "#         ...\n",
    "\n",
    "from modulus.models.layers import get_activation\n",
    "\n",
    "input_dim_grid_nodes = 31\n",
    "input_dim_mesh_nodes = 3\n",
    "input_dim_edges = 4\n",
    "output_dim = 64\n",
    "hidden_dim = 64\n",
    "hidden_layers = 1\n",
    "activation_fn = get_activation(\"silu\")\n",
    "norm_type = \"TELayerNorm\"\n",
    "recompute_activation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.models.gnn_layers.mesh_graph_mlp import MeshGraphMLP\n",
    "\n",
    "grid_node_mlp = MeshGraphMLP(\n",
    "    input_dim=input_dim_grid_nodes,\n",
    "    output_dim=output_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activation_fn=activation_fn,\n",
    "    norm_type=norm_type,\n",
    "    recompute_activation=recompute_activation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeshGraphMLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): LayerNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_node_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3673e+00,  4.9746e-01,  6.1957e-01, -6.8527e-02,  3.3726e-01,\n",
       "          -4.6997e-01,  1.2477e+00, -4.0895e-01,  7.9581e-02,  1.1556e+00,\n",
       "          -1.8389e+00, -6.2278e-01,  1.1993e+00, -5.0889e-01, -6.8851e-01,\n",
       "          -3.0719e-01,  3.1220e-01,  5.4997e-01, -1.1058e+00, -3.4738e-01,\n",
       "          -4.3666e-01, -1.7158e+00,  1.7773e+00, -4.9188e-01,  3.5039e-01,\n",
       "          -1.2541e+00, -6.1864e-01, -1.0331e-01, -9.5205e-01, -3.2043e-01,\n",
       "          -7.0157e-01, -1.3125e+00],\n",
       "         [ 1.6349e+00,  3.1402e-01, -2.7442e-01, -8.7179e-01, -4.3892e-01,\n",
       "          -1.2839e+00, -6.3945e-01, -1.0386e+00, -1.3345e+00,  3.2789e-01,\n",
       "          -1.8072e+00, -8.4228e-01, -2.1458e+00, -1.1730e-01, -1.1539e+00,\n",
       "          -4.0861e-01, -5.6240e-01, -9.9082e-01,  1.0773e+00, -7.0471e-01,\n",
       "          -2.7601e+00, -9.9589e-02,  1.6313e+00, -1.0782e-01, -2.3176e+00,\n",
       "           1.1752e+00, -5.8979e-01, -5.1697e-01,  3.0977e-01, -1.2914e+00,\n",
       "          -1.0224e+00, -8.3342e-01],\n",
       "         [ 1.5534e-01,  1.2178e+00,  6.9715e-01,  4.8151e-01,  1.6281e-01,\n",
       "           2.9447e-01, -2.9098e+00,  6.3905e-01,  3.7011e-01, -1.0503e+00,\n",
       "          -9.0367e-01, -7.8626e-01,  4.9959e-01,  1.1572e+00,  2.3203e-01,\n",
       "          -8.9281e-01, -1.0708e-01,  1.5872e-01,  1.8154e+00,  1.1942e+00,\n",
       "          -5.5297e-01, -3.7329e-01,  2.0531e+00, -4.3713e-01,  9.7285e-01,\n",
       "          -3.5688e-02,  5.8346e-01, -8.5398e-01,  6.8436e-01, -8.2090e-05,\n",
       "          -2.1452e+00,  1.5879e+00]]),\n",
       " torch.Size([3, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_in = torch.randn(3, 32)\n",
    "var_in, var_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x32 and 31x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrid_node_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_in\u001b[49m\u001b[43m)\u001b[49m, grid_node_mlp(var_in)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1714\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1725\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/modulus/models/gnn_layers/mesh_graph_mlp.py:199\u001b[0m, in \u001b[0;36mMeshGraphMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecompute_activation:\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_silu_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_forward(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/modulus/models/gnn_layers/mesh_graph_mlp.py:185\u001b[0m, in \u001b[0;36mMeshGraphMLP.custom_silu_linear_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"forward pass of the MLP where SiLU is recomputed in backward\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m lin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 185\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    187\u001b[0m     lin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m i]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1714\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1725\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x32 and 31x64)"
     ]
    }
   ],
   "source": [
    "grid_node_mlp(var_in), grid_node_mlp(var_in).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
