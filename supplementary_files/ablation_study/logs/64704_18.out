wandb: Starting wandb agent ðŸ•µï¸
2025-03-23 10:57:51,496 - wandb.wandb_agent - INFO - Running runs: []
2025-03-23 10:57:51,821 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-23 10:57:51,822 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 84
	schedule.phase1.iterations: 300
	schedule.phase1.lr_end: 1
	schedule.phase1.lr_start: 0.005
	schedule.phase2.iterations: 1500
	schedule.phase2.lr_objective: 0.001
	schedule.phase3.iterations: 2000
	schedule.phase3.lr: 0.005
	schedule.phase3.rollout_steps_increments: 5
	toggles.data.fix_data_centering: True
	toggles.data.fix_december_gap: True
	toggles.data.fix_sst_data: True
	toggles.data.fix_temporal_info: True
	toggles.data.include_sst_channel: True
	toggles.graph.use_multimesh: False
	toggles.loss.fix_inverse_variance_data: True
	toggles.loss.use_original_variable_weights: True
	toggles.model.include_solar_radiation: True
	toggles.model.include_spatial_info: True
	toggles.model.include_static_data: True
	toggles.model.include_temporal_info: False
2025-03-23 10:57:51,826 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=84 schedule.phase1.iterations=300 schedule.phase1.lr_end=1 schedule.phase1.lr_start=0.005 schedule.phase2.iterations=1500 schedule.phase2.lr_objective=0.001 schedule.phase3.iterations=2000 schedule.phase3.lr=0.005 schedule.phase3.rollout_steps_increments=5 toggles.data.fix_data_centering=True toggles.data.fix_december_gap=True toggles.data.fix_sst_data=True toggles.data.fix_temporal_info=True toggles.data.include_sst_channel=True toggles.graph.use_multimesh=False toggles.loss.fix_inverse_variance_data=True toggles.loss.use_original_variable_weights=True toggles.model.include_solar_radiation=True toggles.model.include_spatial_info=True toggles.model.include_static_data=True toggles.model.include_temporal_info=False
2025-03-23 10:57:56,833 - wandb.wandb_agent - INFO - Running runs: ['oz4p2eod']
[2025-03-23 10:58:16,614][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-23 10:58:16,614][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-23 10:58:16,614][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-23 10:58:16,621][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250323_105817-oz4p2eod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_64731_25/03/23_10:58:16_RUN_01
wandb: â­ï¸ View project at https://wandb.ai/schups/DSM500_FPR
wandb: ðŸ§¹ View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/bv6kdc3l
wandb: ðŸš€ View run at https://wandb.ai/schups/DSM500_FPR/runs/oz4p2eod
[2025-03-23 10:58:18,465][trainer][INFO] - [94mSetting seed to 84[0m
[2025-03-23 10:58:18,533][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-23 10:58:18,534][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-23 10:58:18,534][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-23 10:58:18,536][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-23 10:58:22,432][trainer][INFO] - [94mModel created. Trainable parameters count is 35'246'101[0m
[2025-03-23 10:58:28,171][trainer][INFO] - [92mLoaded train datapipe of size 54'019 samples[0m
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x40016de961a0>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400159f86200>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
[2025-03-23 10:58:29,957][trainer][INFO] - [92mLoaded test datapipe of size 2'912 samples[0m
[2025-03-23 10:58:29,961][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-23 10:58:29,961][main][INFO] - [94mTraining started...[0m
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400160fa2170>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400169c46320>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
[2025-03-23 10:59:24,368][trainer][INFO] - Iteration     1 | Train loss: 0.03 | Time taken:  8.01/46.40/54.41 sec | GPU memory: 73.9 GB | Global sample ID: 45860
[2025-03-23 10:59:24,915][trainer][INFO] - Iteration     2 | Train loss: 0.03 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 53059
[2025-03-23 10:59:25,277][trainer][INFO] - Iteration     3 | Train loss: 0.03 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 30203
[2025-03-23 10:59:25,639][trainer][INFO] - Iteration     4 | Train loss: 0.03 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 21180
[2025-03-23 10:59:26,002][trainer][INFO] - Iteration     5 | Train loss: 0.03 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 35723
[2025-03-23 10:59:26,366][trainer][INFO] - Iteration     6 | Train loss: 0.03 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 27154
[2025-03-23 10:59:26,725][trainer][INFO] - Iteration     7 | Train loss: 0.03 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 15476
[2025-03-23 10:59:27,093][trainer][INFO] - Iteration     8 | Train loss: 0.02 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 8090
[2025-03-23 10:59:27,455][trainer][INFO] - Iteration     9 | Train loss: 0.02 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 43331
[2025-03-23 10:59:27,821][trainer][INFO] - Iteration    10 | Train loss: 0.02 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 2874
[2025-03-23 10:59:28,182][trainer][INFO] - Iteration    11 | Train loss: 0.02 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 2008
[2025-03-23 10:59:28,544][trainer][INFO] - Iteration    12 | Train loss: 0.02 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 11928
[2025-03-23 10:59:28,905][trainer][INFO] - Iteration    13 | Train loss: 0.02 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 30827
[2025-03-23 10:59:29,291][trainer][INFO] - Iteration    14 | Train loss: 0.02 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 52149
[2025-03-23 10:59:29,649][trainer][INFO] - Iteration    15 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 14990
[2025-03-23 10:59:30,007][trainer][INFO] - Iteration    16 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 26934
[2025-03-23 10:59:30,393][trainer][INFO] - Iteration    17 | Train loss: 0.01 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 17494
[2025-03-23 10:59:30,750][trainer][INFO] - Iteration    18 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 12478
[2025-03-23 10:59:31,115][trainer][INFO] - Iteration    19 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 50172
[2025-03-23 10:59:31,535][trainer][INFO] - Iteration    20 | Train loss: 0.01 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 46336
[2025-03-23 10:59:31,892][trainer][INFO] - Iteration    21 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 41035
[2025-03-23 10:59:32,257][trainer][INFO] - Iteration    22 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 1700
[2025-03-23 10:59:32,633][trainer][INFO] - Iteration    23 | Train loss: 0.01 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 38511
[2025-03-23 10:59:33,036][trainer][INFO] - Iteration    24 | Train loss: 0.01 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 10588
[2025-03-23 10:59:33,394][trainer][INFO] - Iteration    25 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 27369
[2025-03-23 10:59:33,761][trainer][INFO] - Iteration    26 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 33913
[2025-03-23 10:59:34,124][trainer][INFO] - Iteration    27 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 34700
[2025-03-23 10:59:34,566][trainer][INFO] - Iteration    28 | Train loss: 0.01 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 14638
[2025-03-23 10:59:34,929][trainer][INFO] - Iteration    29 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 7967
[2025-03-23 10:59:35,328][trainer][INFO] - Iteration    30 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 20441
[2025-03-23 10:59:35,685][trainer][INFO] - Iteration    31 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 39676
[2025-03-23 10:59:36,045][trainer][INFO] - Iteration    32 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 53428
[2025-03-23 10:59:36,406][trainer][INFO] - Iteration    33 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 7604
[2025-03-23 10:59:36,767][trainer][INFO] - Iteration    34 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 50111
[2025-03-23 10:59:37,128][trainer][INFO] - Iteration    35 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 51471
[2025-03-23 10:59:37,490][trainer][INFO] - Iteration    36 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 2022
[2025-03-23 10:59:37,912][trainer][INFO] - Iteration    37 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 24320
[2025-03-23 10:59:38,270][trainer][INFO] - Iteration    38 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 19922
[2025-03-23 10:59:38,635][trainer][INFO] - Iteration    39 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 28337
[2025-03-23 10:59:39,002][trainer][INFO] - Iteration    40 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 47445
[2025-03-23 10:59:39,364][trainer][INFO] - Iteration    41 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 91
[2025-03-23 10:59:39,724][trainer][INFO] - Iteration    42 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 32247
[2025-03-23 10:59:40,091][trainer][INFO] - Iteration    43 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 25422
[2025-03-23 10:59:40,453][trainer][INFO] - Iteration    44 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 23517
[2025-03-23 10:59:40,819][trainer][INFO] - Iteration    45 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 21312
[2025-03-23 10:59:41,182][trainer][INFO] - Iteration    46 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 26228
[2025-03-23 10:59:41,543][trainer][INFO] - Iteration    47 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 44508
[2025-03-23 10:59:41,904][trainer][INFO] - Iteration    48 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 40684
[2025-03-23 10:59:42,335][trainer][INFO] - Iteration    49 | Train loss: 0.00 | Time taken:  0.00/ 0.25/ 0.25 sec | GPU memory: 73.9 GB | Global sample ID: 2416
[2025-03-23 10:59:42,689][trainer][INFO] - Iteration    50 | Train loss: 0.00 | Time taken:  0.00/ 0.26/ 0.26 sec | GPU memory: 73.9 GB | Global sample ID: 34267
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=False', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=True', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=True', 'toggles.model.include_temporal_info=False']
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=False', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=True', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=True', 'toggles.model.include_temporal_info=False']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 377, in _get_outputs
    self._schedule_runs(False)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 420, in _schedule_runs
    p.schedule_run()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1183, in schedule_run
    self._prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1342, in _prefetch
    self._legacy_interleaved_prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1365, in _legacy_interleaved_prefetch
    self._iter_setup()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1675, in _iter_setup
    iters, success = self._run_input_callbacks()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1724, in _run_input_callbacks
    batch.feed()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 138, in feed
    self._group.feed(self._pipepline, self._data, self._batch_size)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 292, in feed
    pipeline._feed_input(
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1057, in _feed_input
    self._pipe.SetExternalTensorInput(
RuntimeError: Could not find an input operator with name "__ExternalSource_4"

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 377, in _get_outputs
    self._schedule_runs(False)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 420, in _schedule_runs
    p.schedule_run()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1183, in schedule_run
    self._prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1342, in _prefetch
    self._legacy_interleaved_prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1365, in _legacy_interleaved_prefetch
    self._iter_setup()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1675, in _iter_setup
    iters, success = self._run_input_callbacks()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1724, in _run_input_callbacks
    batch.feed()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 138, in feed
    self._group.feed(self._pipepline, self._data, self._batch_size)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 292, in feed
    pipeline._feed_input(
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1057, in _feed_input
    self._pipe.SetExternalTensorInput(
RuntimeError: Could not find an input operator with name "__ExternalSource_4"

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=False', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=True', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=True', 'toggles.model.include_temporal_info=False']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 377, in _get_outputs
    self._schedule_runs(False)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 420, in _schedule_runs
    p.schedule_run()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1183, in schedule_run
    self._prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1342, in _prefetch
    self._legacy_interleaved_prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1365, in _legacy_interleaved_prefetch
    self._iter_setup()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1675, in _iter_setup
    iters, success = self._run_input_callbacks()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1724, in _run_input_callbacks
    batch.feed()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 138, in feed
    self._group.feed(self._pipepline, self._data, self._batch_size)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 292, in feed
    pipeline._feed_input(
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1057, in _feed_input
    self._pipe.SetExternalTensorInput(
RuntimeError: Could not find an input operator with name "__ExternalSource_4"

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=False', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=True', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=True', 'toggles.model.include_temporal_info=False']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 377, in _get_outputs
    self._schedule_runs(False)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 420, in _schedule_runs
    p.schedule_run()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1183, in schedule_run
    self._prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1342, in _prefetch
    self._legacy_interleaved_prefetch()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1365, in _legacy_interleaved_prefetch
    self._iter_setup()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1675, in _iter_setup
    iters, success = self._run_input_callbacks()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1724, in _run_input_callbacks
    batch.feed()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 138, in feed
    self._group.feed(self._pipepline, self._data, self._batch_size)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/external_source.py", line 292, in feed
    pipeline._feed_input(
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1057, in _feed_input
    self._pipe.SetExternalTensorInput(
RuntimeError: Could not find an input operator with name "__ExternalSource_4"

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.030 MB of 0.030 MB uploadedW0323 10:59:53.526000 254167 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 254170 closing signal SIGTERM
W0323 10:59:53.530000 254167 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 254171 closing signal SIGTERM
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0323 10:59:54.100000 254167 torch/distributed/elastic/multiprocessing/api.py:862] failed (exitcode: 1) local_rank: 2 (pid: 254172) of binary: /iopsstor/scratch/cscs/stefschu/DSM500_FPR/env/venv_arm64/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 923, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_graphcast.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-03-23_10:59:53
  host      : nid006017
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 254173)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-23_10:59:53
  host      : nid006017
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 254172)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
2025-03-23 10:59:55,068 - wandb.wandb_agent - INFO - Cleaning up finished run: oz4p2eod
wandb: Terminating and syncing runs. Press ctrl-c to kill.
