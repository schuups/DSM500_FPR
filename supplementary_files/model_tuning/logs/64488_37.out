wandb: Starting wandb agent 🕵️
2025-03-23 02:43:54,081 - wandb.wandb_agent - INFO - Running runs: []
2025-03-23 02:43:54,417 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-23 02:43:54,417 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 84
	schedule.phase1.iterations: 300
	schedule.phase1.lr_end: 1
	schedule.phase1.lr_start: 0.005
	schedule.phase2.iterations: 1500
	schedule.phase2.lr_objective: 0.001
	schedule.phase3.iterations: 2000
	schedule.phase3.lr: 0.005
	schedule.phase3.rollout_steps_increments: 5
	toggles.data.fix_data_centering: True
	toggles.data.fix_december_gap: True
	toggles.data.fix_sst_data: True
	toggles.data.fix_temporal_info: True
	toggles.data.include_sst_channel: True
	toggles.graph.use_multimesh: True
	toggles.loss.fix_inverse_variance_data: True
	toggles.loss.use_original_variable_weights: False
	toggles.model.include_solar_radiation: True
	toggles.model.include_spatial_info: True
	toggles.model.include_static_data: False
	toggles.model.include_temporal_info: False
2025-03-23 02:43:54,421 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=84 schedule.phase1.iterations=300 schedule.phase1.lr_end=1 schedule.phase1.lr_start=0.005 schedule.phase2.iterations=1500 schedule.phase2.lr_objective=0.001 schedule.phase3.iterations=2000 schedule.phase3.lr=0.005 schedule.phase3.rollout_steps_increments=5 toggles.data.fix_data_centering=True toggles.data.fix_december_gap=True toggles.data.fix_sst_data=True toggles.data.fix_temporal_info=True toggles.data.include_sst_channel=True toggles.graph.use_multimesh=True toggles.loss.fix_inverse_variance_data=True toggles.loss.use_original_variable_weights=False toggles.model.include_solar_radiation=True toggles.model.include_spatial_info=True toggles.model.include_static_data=False toggles.model.include_temporal_info=False
2025-03-23 02:43:59,424 - wandb.wandb_agent - INFO - Running runs: ['x82ewrqg']
[2025-03-23 02:44:18,725][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-23 02:44:18,725][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-23 02:44:18,725][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-23 02:44:18,727][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250323_024419-x82ewrqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_64548_25/03/23_02:44:18_RUN_01
wandb: ⭐️ View project at https://wandb.ai/schups/DSM500_FPR
wandb: 🧹 View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/bv6kdc3l
wandb: 🚀 View run at https://wandb.ai/schups/DSM500_FPR/runs/x82ewrqg
[2025-03-23 02:44:20,566][trainer][INFO] - [94mSetting seed to 84[0m
[2025-03-23 02:44:20,617][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-23 02:44:20,651][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-23 02:44:20,651][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-23 02:44:20,652][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-23 02:44:25,044][trainer][INFO] - [94mModel created. Trainable parameters count is 35'245'077[0m
[2025-03-23 02:44:30,504][trainer][INFO] - [92mLoaded train datapipe of size 54'019 samples[0m
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x40016e7864d0>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x4001660d6620>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400189b26770>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
[2025-03-23 02:44:31,617][trainer][INFO] - [92mLoaded test datapipe of size 2'912 samples[0m
[2025-03-23 02:44:31,950][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-23 02:44:31,960][main][INFO] - [94mTraining started...[0m
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400178900160>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
[2025-03-23 02:45:26,995][trainer][INFO] - Iteration     1 | Train loss: 0.03 | Time taken:  8.40/46.63/55.03 sec | GPU memory: 79.1 GB | Global sample ID: 45860
[2025-03-23 02:45:27,630][trainer][INFO] - Iteration     2 | Train loss: 0.03 | Time taken:  0.00/ 0.42/ 0.42 sec | GPU memory: 82.1 GB | Global sample ID: 53059
[2025-03-23 02:45:28,108][trainer][INFO] - Iteration     3 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 30203
[2025-03-23 02:45:28,520][trainer][INFO] - Iteration     4 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 21180
[2025-03-23 02:45:28,914][trainer][INFO] - Iteration     5 | Train loss: 0.02 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 35723
[2025-03-23 02:45:29,320][trainer][INFO] - Iteration     6 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 27154
[2025-03-23 02:45:29,937][trainer][INFO] - Iteration     7 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15476
[2025-03-23 02:45:30,331][trainer][INFO] - Iteration     8 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 8090
[2025-03-23 02:45:30,728][trainer][INFO] - Iteration     9 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 43331
[2025-03-23 02:45:31,182][trainer][INFO] - Iteration    10 | Train loss: 0.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2874
[2025-03-23 02:45:31,736][trainer][INFO] - Iteration    11 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2008
[2025-03-23 02:45:32,133][trainer][INFO] - Iteration    12 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 11928
[2025-03-23 02:45:32,529][trainer][INFO] - Iteration    13 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 30827
[2025-03-23 02:45:32,925][trainer][INFO] - Iteration    14 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 52149
[2025-03-23 02:45:33,316][trainer][INFO] - Iteration    15 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 14990
[2025-03-23 02:45:33,749][trainer][INFO] - Iteration    16 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 26934
[2025-03-23 02:45:34,198][trainer][INFO] - Iteration    17 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 17494
[2025-03-23 02:45:34,607][trainer][INFO] - Iteration    18 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 12478
[2025-03-23 02:45:35,002][trainer][INFO] - Iteration    19 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 50172
[2025-03-23 02:45:35,520][trainer][INFO] - Iteration    20 | Train loss: 0.01 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 46336
[2025-03-23 02:45:35,914][trainer][INFO] - Iteration    21 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 41035
[2025-03-23 02:45:36,365][trainer][INFO] - Iteration    22 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 1700
[2025-03-23 02:45:36,772][trainer][INFO] - Iteration    23 | Train loss: 0.01 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 38511
[2025-03-23 02:45:37,170][trainer][INFO] - Iteration    24 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 10588
[2025-03-23 02:45:37,569][trainer][INFO] - Iteration    25 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 27369
[2025-03-23 02:45:37,966][trainer][INFO] - Iteration    26 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 33913
[2025-03-23 02:45:38,362][trainer][INFO] - Iteration    27 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 34700
[2025-03-23 02:45:38,762][trainer][INFO] - Iteration    28 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 14638
[2025-03-23 02:45:39,160][trainer][INFO] - Iteration    29 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 7967
[2025-03-23 02:45:39,558][trainer][INFO] - Iteration    30 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 20441
[2025-03-23 02:45:39,957][trainer][INFO] - Iteration    31 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 39676
[2025-03-23 02:45:40,355][trainer][INFO] - Iteration    32 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 53428
[2025-03-23 02:45:40,758][trainer][INFO] - Iteration    33 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 7604
[2025-03-23 02:45:41,156][trainer][INFO] - Iteration    34 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 50111
[2025-03-23 02:45:41,669][trainer][INFO] - Iteration    35 | Train loss: 0.00 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 51471
[2025-03-23 02:45:42,069][trainer][INFO] - Iteration    36 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 2022
[2025-03-23 02:45:42,468][trainer][INFO] - Iteration    37 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 24320
[2025-03-23 02:45:42,888][trainer][INFO] - Iteration    38 | Train loss: 0.00 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 19922
[2025-03-23 02:45:43,287][trainer][INFO] - Iteration    39 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 28337
[2025-03-23 02:45:43,688][trainer][INFO] - Iteration    40 | Train loss: 0.00 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 47445
[2025-03-23 02:45:44,086][trainer][INFO] - Iteration    41 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 91
[2025-03-23 02:45:44,482][trainer][INFO] - Iteration    42 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 32247
[2025-03-23 02:45:44,879][trainer][INFO] - Iteration    43 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 25422
[2025-03-23 02:45:45,277][trainer][INFO] - Iteration    44 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 23517
[2025-03-23 02:45:45,680][trainer][INFO] - Iteration    45 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.30 sec | GPU memory: 82.1 GB | Global sample ID: 21312
[2025-03-23 02:45:46,078][trainer][INFO] - Iteration    46 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 26228
[2025-03-23 02:45:46,480][trainer][INFO] - Iteration    47 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 44508
[2025-03-23 02:45:47,041][trainer][INFO] - Iteration    48 | Train loss: 0.00 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 40684
[2025-03-23 02:45:47,466][trainer][INFO] - Iteration    49 | Train loss: 0.00 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2416
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400189b6f1c0>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x40016e7d2e90>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400166122fb0>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
[2025-03-23 02:45:47,866][trainer][INFO] - Iteration    50 | Train loss: 0.00 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 34267
/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py:912: Warning: The external source node '<modulus.datapipes.era5_hdf5.ERA5DaliExternalSource object at 0x400178a1e380>' produces 6 outputs, but the output at the index 4 is not used. For best performance, adjust your callback so that it computes only the needed outputs.
  warnings.warn(
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=True', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=False', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=False', 'toggles.model.include_temporal_info=False']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 934805504 bytes on device 3.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=84', 'schedule.phase1.iterations=300', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=2000', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=5', 'toggles.data.fix_data_centering=True', 'toggles.data.fix_december_gap=True', 'toggles.data.fix_sst_data=True', 'toggles.data.fix_temporal_info=True', 'toggles.data.include_sst_channel=True', 'toggles.graph.use_multimesh=True', 'toggles.loss.fix_inverse_variance_data=True', 'toggles.loss.use_original_variable_weights=False', 'toggles.model.include_solar_radiation=True', 'toggles.model.include_spatial_info=True', 'toggles.model.include_static_data=False', 'toggles.model.include_temporal_info=False']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 934805504 bytes on device 1.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-03-23 02:45:58,210][trainer][INFO] - [94mIteration    50 | Test MSE:   0.75 | Time taken:  9.03/ 1.26/10.30 sec | GPU memory: 82.1 GB | Global sample ID: 2627[0m
[rank1]:[E323 02:55:56.705007433 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600028 milliseconds before timing out.
[rank1]:[E323 02:55:56.706817612 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank1]:[E323 02:55:56.706823339 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank1]:[E323 02:55:56.706829803 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E323 02:55:56.708505938 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
[rank3]:[E323 02:55:56.708682156 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank3]:[E323 02:55:56.708692747 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 3] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank3]:[E323 02:55:56.708701131 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E323 02:55:58.220393222 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600002 milliseconds before timing out.
[rank2]:[E323 02:55:58.220492483 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank2]:[E323 02:55:58.220497699 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 2] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank2]:[E323 02:55:58.220501923 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 02:55:58.392702399 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
[rank0]:[E323 02:55:58.392751965 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E323 02:55:58.392756189 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E323 02:55:58.392759805 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 02:55:58.492869692 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=202, OpType=ALLREDUCE, NumelIn=13139968, NumelOut=13139968, Timeout(ms)=600000) ran for 600095 milliseconds before timing out.
[rank0]:[E323 02:55:58.492881595 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E323 02:55:58.492884187 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E323 02:55:58.492886747 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 02:55:58.592965148 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=203, OpType=ALLREDUCE, NumelIn=13398528, NumelOut=13398528, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
[rank0]:[E323 02:55:58.592974428 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E323 02:55:58.592977084 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E323 02:55:58.592979676 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 02:55:58.592990492 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=204, OpType=ALLREDUCE, NumelIn=8169472, NumelOut=8169472, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
[rank0]:[E323 02:55:58.592995931 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E323 02:55:58.592998171 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E323 02:55:58.593000379 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E323 03:08:15.841258332 ProcessGroupNCCL.cpp:1496] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
slurmstepd: error: *** STEP 64548.0 ON nid007209 CANCELLED AT 2025-03-23T03:13:29 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 64548 ON nid007209 CANCELLED AT 2025-03-23T03:13:29 DUE TO TIME LIMIT ***
