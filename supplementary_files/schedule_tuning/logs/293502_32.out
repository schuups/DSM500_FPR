wandb: Starting wandb agent 🕵️
2025-03-22 22:51:03,272 - wandb.wandb_agent - INFO - Running runs: []
2025-03-22 22:51:03,742 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-22 22:51:03,742 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 42
	schedule.phase1.iterations: 500
	schedule.phase1.lr_end: 0.01
	schedule.phase1.lr_start: 0.005
	schedule.phase2.iterations: 1000
	schedule.phase2.lr_objective: 0.001
	schedule.phase3.iterations: 100
	schedule.phase3.lr: 0.001
	schedule.phase3.rollout_steps_increments: 1
2025-03-22 22:51:03,745 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=42 schedule.phase1.iterations=500 schedule.phase1.lr_end=0.01 schedule.phase1.lr_start=0.005 schedule.phase2.iterations=1000 schedule.phase2.lr_objective=0.001 schedule.phase3.iterations=100 schedule.phase3.lr=0.001 schedule.phase3.rollout_steps_increments=1
2025-03-22 22:51:08,753 - wandb.wandb_agent - INFO - Running runs: ['8ef3wqh2']
[2025-03-22 22:51:28,789][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-22 22:51:28,789][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-22 22:51:28,791][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-22 22:51:28,794][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250322_225129-8ef3wqh2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_293600_25/03/22_22:51:28_RUN_01
wandb: ⭐️ View project at https://wandb.ai/schups/DSM500_FPR
wandb: 🧹 View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/mpgn79e1
wandb: 🚀 View run at https://wandb.ai/schups/DSM500_FPR/runs/8ef3wqh2
[2025-03-22 22:51:30,580][trainer][INFO] - [94mSetting seed to 42[0m
[2025-03-22 22:51:30,633][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-22 22:51:30,633][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-22 22:51:30,633][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-22 22:51:30,635][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-22 22:51:35,027][trainer][INFO] - [94mModel created. Trainable parameters count is 35'248'149[0m
[2025-03-22 22:51:40,724][trainer][INFO] - [92mLoaded train datapipe of size 53'947 samples[0m
[2025-03-22 22:51:42,483][trainer][INFO] - [92mLoaded test datapipe of size 2'903 samples[0m
[2025-03-22 22:51:42,486][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-22 22:51:42,486][main][INFO] - [94mTraining started...[0m
[2025-03-22 22:52:30,318][trainer][INFO] - Iteration     1 | Train loss: 3.98 | Time taken:  7.10/40.73/47.83 sec | GPU memory: 79.1 GB | Global sample ID: 35335
[2025-03-22 22:52:30,911][trainer][INFO] - Iteration     2 | Train loss: 3.70 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 39950
[2025-03-22 22:52:31,299][trainer][INFO] - Iteration     3 | Train loss: 3.52 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 17002
[2025-03-22 22:52:31,874][trainer][INFO] - Iteration     4 | Train loss: 3.61 | Time taken:  0.00/ 0.31/ 0.31 sec | GPU memory: 82.1 GB | Global sample ID: 19918
[2025-03-22 22:52:32,322][trainer][INFO] - Iteration     5 | Train loss: 3.72 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15961
[2025-03-22 22:52:33,147][trainer][INFO] - Iteration     6 | Train loss: 3.83 | Time taken:  0.00/ 0.71/ 0.71 sec | GPU memory: 82.1 GB | Global sample ID: 11951
[2025-03-22 22:52:33,536][trainer][INFO] - Iteration     7 | Train loss: 3.88 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 13377
[2025-03-22 22:52:33,932][trainer][INFO] - Iteration     8 | Train loss: 3.61 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 25632
[2025-03-22 22:52:34,333][trainer][INFO] - Iteration     9 | Train loss: 3.80 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 18934
[2025-03-22 22:52:34,733][trainer][INFO] - Iteration    10 | Train loss: 3.73 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 15906
[2025-03-22 22:52:35,183][trainer][INFO] - Iteration    11 | Train loss: 3.69 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 46823
[2025-03-22 22:52:35,581][trainer][INFO] - Iteration    12 | Train loss: 3.64 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15819
[2025-03-22 22:52:35,976][trainer][INFO] - Iteration    13 | Train loss: 3.43 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 48235
[2025-03-22 22:52:36,372][trainer][INFO] - Iteration    14 | Train loss: 3.58 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 16177
[2025-03-22 22:52:36,768][trainer][INFO] - Iteration    15 | Train loss: 3.57 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9930
[2025-03-22 22:52:37,168][trainer][INFO] - Iteration    16 | Train loss: 3.00 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 41644
[2025-03-22 22:52:37,566][trainer][INFO] - Iteration    17 | Train loss: 3.10 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3496
[2025-03-22 22:52:37,956][trainer][INFO] - Iteration    18 | Train loss: 3.32 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 5381
[2025-03-22 22:52:38,347][trainer][INFO] - Iteration    19 | Train loss: 2.97 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 14034
[2025-03-22 22:52:38,737][trainer][INFO] - Iteration    20 | Train loss: 2.99 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 13919
[2025-03-22 22:52:39,133][trainer][INFO] - Iteration    21 | Train loss: 2.95 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 50601
[2025-03-22 22:52:39,542][trainer][INFO] - Iteration    22 | Train loss: 3.04 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3080
[2025-03-22 22:52:40,068][trainer][INFO] - Iteration    23 | Train loss: 3.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 37483
[2025-03-22 22:52:40,457][trainer][INFO] - Iteration    24 | Train loss: 3.23 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 20487
[2025-03-22 22:52:40,848][trainer][INFO] - Iteration    25 | Train loss: 2.84 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 50335
[2025-03-22 22:52:41,516][trainer][INFO] - Iteration    26 | Train loss: 2.96 | Time taken:  0.29/ 0.28/ 0.56 sec | GPU memory: 82.1 GB | Global sample ID: 33107
[2025-03-22 22:52:41,913][trainer][INFO] - Iteration    27 | Train loss: 2.73 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 8050
[2025-03-22 22:52:42,306][trainer][INFO] - Iteration    28 | Train loss: 2.81 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 6767
[2025-03-22 22:52:42,698][trainer][INFO] - Iteration    29 | Train loss: 2.97 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 17381
[2025-03-22 22:52:43,091][trainer][INFO] - Iteration    30 | Train loss: 2.82 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 18720
[2025-03-22 22:52:43,492][trainer][INFO] - Iteration    31 | Train loss: 2.75 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3462
[2025-03-22 22:52:43,885][trainer][INFO] - Iteration    32 | Train loss: 2.77 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 43268
[2025-03-22 22:52:44,299][trainer][INFO] - Iteration    33 | Train loss: 2.86 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 10175
[2025-03-22 22:52:44,692][trainer][INFO] - Iteration    34 | Train loss: 2.56 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 15632
[2025-03-22 22:52:45,095][trainer][INFO] - Iteration    35 | Train loss: 2.56 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 25503
[2025-03-22 22:52:45,493][trainer][INFO] - Iteration    36 | Train loss: 2.73 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 1375
[2025-03-22 22:52:45,929][trainer][INFO] - Iteration    37 | Train loss: 2.59 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 17910
[2025-03-22 22:52:46,322][trainer][INFO] - Iteration    38 | Train loss: 2.40 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 33040
[2025-03-22 22:52:46,727][trainer][INFO] - Iteration    39 | Train loss: 2.53 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 44623
[2025-03-22 22:52:47,201][trainer][INFO] - Iteration    40 | Train loss: 2.42 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 38909
[2025-03-22 22:52:47,593][trainer][INFO] - Iteration    41 | Train loss: 2.61 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 37918
[2025-03-22 22:52:48,029][trainer][INFO] - Iteration    42 | Train loss: 2.45 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9131
[2025-03-22 22:52:48,421][trainer][INFO] - Iteration    43 | Train loss: 2.30 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 6797
[2025-03-22 22:52:48,814][trainer][INFO] - Iteration    44 | Train loss: 2.32 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9661
[2025-03-22 22:52:49,212][trainer][INFO] - Iteration    45 | Train loss: 2.19 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 38496
[2025-03-22 22:52:49,608][trainer][INFO] - Iteration    46 | Train loss: 2.20 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 38616
[2025-03-22 22:52:49,998][trainer][INFO] - Iteration    47 | Train loss: 2.28 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 30779
[2025-03-22 22:52:50,388][trainer][INFO] - Iteration    48 | Train loss: 2.29 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 20689
[2025-03-22 22:52:50,781][trainer][INFO] - Iteration    49 | Train loss: 2.10 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 47508
[2025-03-22 22:52:51,207][trainer][INFO] - Iteration    50 | Train loss: 2.22 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 14450
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=42', 'schedule.phase1.iterations=500', 'schedule.phase1.lr_end=0.01', 'schedule.phase1.lr_start=0.005', 'schedule.phase2.iterations=1000', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=100', 'schedule.phase3.lr=0.001', 'schedule.phase3.rollout_steps_increments=1']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 84, in main
    mse, global_sample_id = trainer.test(test_sample)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/utils/trainer.py", line 228, in test
    output = self.model(model_input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/graph_cast_net.py", line 219, in forward
    grid_node_feats_decoded = self.checkpoint_filter(partial(
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/graph_cast_net.py", line 253, in checkpoint_filter
    return partial_function()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/encoder_decoder.py", line 128, in forward
    edge_feature = self.edge_mlp(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/mlp.py", line 141, in forward
    mlp_sum = sum_efeat(
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/utils.py", line 106, in sum_efeat
    src_idx, dst_idx = graph.edges()
  File "/usr/local/lib/python3.10/dist-packages/dgl/view.py", line 179, in __call__
    return self._graph.all_edges(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py", line 3591, in all_edges
    src, dst, eid = self._graph.edges(self.get_etype_id(etype), order)
  File "/usr/local/lib/python3.10/dist-packages/dgl/heterograph_index.py", line 696, in edges
    edge_array = _CAPI_DGLHeteroEdges(self, int(etype), order)
  File "dgl/_ffi/_cython/./function.pxi", line 295, in dgl._ffi._cy3.core.FunctionBase.__call__
  File "dgl/_ffi/_cython/./function.pxi", line 227, in dgl._ffi._cy3.core.FuncCall
  File "dgl/_ffi/_cython/./function.pxi", line 217, in dgl._ffi._cy3.core.FuncCall3
dgl._ffi.base.DGLError: [22:52:59] /workspace/ktangsali/dgl/src/runtime/cuda/cuda_device_api.cc:117: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory
Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x58) [0x4000f2d51418]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DGLContext, unsigned long, unsigned long, DGLDataType)+0x220) [0x4000f37ffb64]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DGLDataType, DGLContext)+0xd8) [0x4000f33deac4]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::NewIdArray(long, DGLContext, unsigned char)+0x98) [0x4000f2d1d8c0]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::NDArray dgl::aten::impl::Range<(DGLDeviceType)2, int>(int, int, DGLContext)+0xc8) [0x4000f383ce4c]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::Range(long, long, unsigned char, DGLContext)+0x168) [0x4000f2d1dc58]
  [bt] (6) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::UnitGraph::COO::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x1d8) [0x4000f376c818]
  [bt] (7) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::UnitGraph::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x1c4) [0x4000f375e298]
  [bt] (8) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::HeteroGraph::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x78) [0x4000f34f221c]



Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.030 MB of 0.030 MB uploadedwandb: \ 0.030 MB of 0.053 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            learning_rate ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            training_loss █▇▆▇▇█▇▇▇▇▆▆▄▅▅▄▄▄▄▅▄▃▃▄▃▃▄▂▃▃▂▂▃▂▁▁▁▁▁▁
wandb:            training_time █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: training_time_dataloader █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      training_time_model █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:            learning_rate 1e-05
wandb:            training_loss 2.21661
wandb:            training_time 0.2862
wandb: training_time_dataloader 0.00218
wandb:      training_time_model 0.28402
wandb: 
wandb: 🚀 View run Baseline_293600_25/03/22_22:51:28_RUN_01 at: https://wandb.ai/schups/DSM500_FPR/runs/8ef3wqh2
wandb: ⭐️ View project at: https://wandb.ai/schups/DSM500_FPR
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/wandb/run-20250322_225129-8ef3wqh2/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
W0322 22:53:10.386000 229229 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 229233 closing signal SIGTERM
W0322 22:53:10.389000 229229 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 229234 closing signal SIGTERM
W0322 22:53:10.399000 229229 torch/distributed/elastic/multiprocessing/api.py:890] Sending process 229235 closing signal SIGTERM
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0322 22:53:11.292000 229229 torch/distributed/elastic/multiprocessing/api.py:862] failed (exitcode: 1) local_rank: 0 (pid: 229232) of binary: /iopsstor/scratch/cscs/stefschu/DSM500_FPR/env/venv_arm64/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 923, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_graphcast.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-22_22:53:10
  host      : nid005017
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 229232)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
2025-03-22 22:53:12,208 - wandb.wandb_agent - INFO - Cleaning up finished run: 8ef3wqh2
wandb: Terminating and syncing runs. Press ctrl-c to kill.
