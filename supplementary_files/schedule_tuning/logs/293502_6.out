wandb: Starting wandb agent 🕵️
2025-03-22 21:14:15,075 - wandb.wandb_agent - INFO - Running runs: []
2025-03-22 21:14:15,319 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-22 21:14:15,323 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 21
	schedule.phase1.iterations: 2000
	schedule.phase1.lr_end: 1
	schedule.phase1.lr_start: 0.001
	schedule.phase2.iterations: 500
	schedule.phase2.lr_objective: 0.001
	schedule.phase3.iterations: 100
	schedule.phase3.lr: 0.005
	schedule.phase3.rollout_steps_increments: 2
2025-03-22 21:14:15,327 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=21 schedule.phase1.iterations=2000 schedule.phase1.lr_end=1 schedule.phase1.lr_start=0.001 schedule.phase2.iterations=500 schedule.phase2.lr_objective=0.001 schedule.phase3.iterations=100 schedule.phase3.lr=0.005 schedule.phase3.rollout_steps_increments=2
2025-03-22 21:14:20,335 - wandb.wandb_agent - INFO - Running runs: ['7yg0oqwc']
[2025-03-22 21:14:40,051][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-22 21:14:40,052][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-22 21:14:40,052][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-22 21:14:40,059][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250322_211440-7yg0oqwc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_293508_25/03/22_21:14:40_RUN_01
wandb: ⭐️ View project at https://wandb.ai/schups/DSM500_FPR
wandb: 🧹 View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/mpgn79e1
wandb: 🚀 View run at https://wandb.ai/schups/DSM500_FPR/runs/7yg0oqwc
[2025-03-22 21:14:41,878][trainer][INFO] - [94mSetting seed to 21[0m
[2025-03-22 21:14:41,925][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-22 21:14:41,925][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-22 21:14:41,926][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-22 21:14:41,927][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-22 21:14:46,313][trainer][INFO] - [94mModel created. Trainable parameters count is 35'248'149[0m
[2025-03-22 21:14:52,014][trainer][INFO] - [92mLoaded train datapipe of size 53'947 samples[0m
[2025-03-22 21:14:53,826][trainer][INFO] - [92mLoaded test datapipe of size 2'903 samples[0m
[2025-03-22 21:14:53,829][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-22 21:14:53,829][main][INFO] - [94mTraining started...[0m
[2025-03-22 21:15:49,428][trainer][INFO] - Iteration     1 | Train loss: 4.87 | Time taken:  7.30/48.30/55.60 sec | GPU memory: 79.1 GB | Global sample ID: 3374
[2025-03-22 21:15:49,817][trainer][INFO] - Iteration     2 | Train loss: 4.97 | Time taken:  0.00/ 0.27/ 0.27 sec | GPU memory: 82.1 GB | Global sample ID: 30756
[2025-03-22 21:15:50,204][trainer][INFO] - Iteration     3 | Train loss: 4.82 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 1692
[2025-03-22 21:15:51,045][trainer][INFO] - Iteration     4 | Train loss: 4.97 | Time taken:  0.00/ 0.30/ 0.30 sec | GPU memory: 82.1 GB | Global sample ID: 14842
[2025-03-22 21:15:51,428][trainer][INFO] - Iteration     5 | Train loss: 4.83 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 41459
[2025-03-22 21:15:51,828][trainer][INFO] - Iteration     6 | Train loss: 4.79 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 22876
[2025-03-22 21:15:52,213][trainer][INFO] - Iteration     7 | Train loss: 4.80 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 36064
[2025-03-22 21:15:52,596][trainer][INFO] - Iteration     8 | Train loss: 4.63 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 37044
[2025-03-22 21:15:53,199][trainer][INFO] - Iteration     9 | Train loss: 4.93 | Time taken:  0.00/ 0.43/ 0.43 sec | GPU memory: 82.1 GB | Global sample ID: 40746
[2025-03-22 21:15:53,583][trainer][INFO] - Iteration    10 | Train loss: 4.71 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 33084
[2025-03-22 21:15:53,971][trainer][INFO] - Iteration    11 | Train loss: 4.51 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 45618
[2025-03-22 21:15:54,356][trainer][INFO] - Iteration    12 | Train loss: 4.53 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 30078
[2025-03-22 21:15:54,835][trainer][INFO] - Iteration    13 | Train loss: 4.74 | Time taken:  0.00/ 0.27/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 5703
[2025-03-22 21:15:55,217][trainer][INFO] - Iteration    14 | Train loss: 4.43 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15297
[2025-03-22 21:15:55,702][trainer][INFO] - Iteration    15 | Train loss: 4.61 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 4167
[2025-03-22 21:15:56,091][trainer][INFO] - Iteration    16 | Train loss: 4.20 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 16140
[2025-03-22 21:15:56,479][trainer][INFO] - Iteration    17 | Train loss: 4.09 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 38984
[2025-03-22 21:15:56,867][trainer][INFO] - Iteration    18 | Train loss: 3.99 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 48331
[2025-03-22 21:15:57,355][trainer][INFO] - Iteration    19 | Train loss: 3.84 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 34640
[2025-03-22 21:15:57,737][trainer][INFO] - Iteration    20 | Train loss: 3.57 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 3219
[2025-03-22 21:15:58,124][trainer][INFO] - Iteration    21 | Train loss: 3.44 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15494
[2025-03-22 21:15:58,588][trainer][INFO] - Iteration    22 | Train loss: 3.28 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 49159
[2025-03-22 21:15:58,973][trainer][INFO] - Iteration    23 | Train loss: 3.06 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40366
[2025-03-22 21:15:59,357][trainer][INFO] - Iteration    24 | Train loss: 3.00 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2423
[2025-03-22 21:15:59,751][trainer][INFO] - Iteration    25 | Train loss: 2.89 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 3703
[2025-03-22 21:16:00,136][trainer][INFO] - Iteration    26 | Train loss: 2.83 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 32159
[2025-03-22 21:16:00,528][trainer][INFO] - Iteration    27 | Train loss: 2.70 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40868
[2025-03-22 21:16:00,989][trainer][INFO] - Iteration    28 | Train loss: 2.44 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 26244
[2025-03-22 21:16:01,492][trainer][INFO] - Iteration    29 | Train loss: 2.33 | Time taken:  0.00/ 0.27/ 0.27 sec | GPU memory: 82.1 GB | Global sample ID: 3851
[2025-03-22 21:16:01,872][trainer][INFO] - Iteration    30 | Train loss: 2.13 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 31380
[2025-03-22 21:16:02,299][trainer][INFO] - Iteration    31 | Train loss: 2.01 | Time taken:  0.00/ 0.27/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 45111
[2025-03-22 21:16:02,688][trainer][INFO] - Iteration    32 | Train loss: 1.65 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 48735
[2025-03-22 21:16:03,074][trainer][INFO] - Iteration    33 | Train loss: 1.48 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 52716
[2025-03-22 21:16:03,461][trainer][INFO] - Iteration    34 | Train loss: 1.19 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 35442
[2025-03-22 21:16:03,848][trainer][INFO] - Iteration    35 | Train loss: 1.13 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 28555
[2025-03-22 21:16:04,244][trainer][INFO] - Iteration    36 | Train loss: 0.99 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 52525
[2025-03-22 21:16:04,644][trainer][INFO] - Iteration    37 | Train loss: 0.68 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 53105
[2025-03-22 21:16:05,125][trainer][INFO] - Iteration    38 | Train loss: 0.70 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2872
[2025-03-22 21:16:05,516][trainer][INFO] - Iteration    39 | Train loss: 0.55 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 46733
[2025-03-22 21:16:05,903][trainer][INFO] - Iteration    40 | Train loss: 0.51 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 26205
[2025-03-22 21:16:06,292][trainer][INFO] - Iteration    41 | Train loss: 0.41 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 50900
[2025-03-22 21:16:06,748][trainer][INFO] - Iteration    42 | Train loss: 0.33 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40652
[2025-03-22 21:16:07,133][trainer][INFO] - Iteration    43 | Train loss: 0.37 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 20630
[2025-03-22 21:16:07,614][trainer][INFO] - Iteration    44 | Train loss: 0.30 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 43052
[2025-03-22 21:16:08,056][trainer][INFO] - Iteration    45 | Train loss: 0.32 | Time taken:  0.00/ 0.27/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 5155
[2025-03-22 21:16:08,451][trainer][INFO] - Iteration    46 | Train loss: 0.31 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 28846
[2025-03-22 21:16:08,947][trainer][INFO] - Iteration    47 | Train loss: 0.32 | Time taken:  0.00/ 0.27/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 43929
[2025-03-22 21:16:09,335][trainer][INFO] - Iteration    48 | Train loss: 0.28 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 12607
[2025-03-22 21:16:09,721][trainer][INFO] - Iteration    49 | Train loss: 0.28 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 30861
[2025-03-22 21:16:10,174][trainer][INFO] - Iteration    50 | Train loss: 0.26 | Time taken:  0.00/ 0.27/ 0.27 sec | GPU memory: 82.1 GB | Global sample ID: 45256
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=21', 'schedule.phase1.iterations=2000', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.001', 'schedule.phase2.iterations=500', 'schedule.phase2.lr_objective=0.001', 'schedule.phase3.iterations=100', 'schedule.phase3.lr=0.005', 'schedule.phase3.rollout_steps_increments=2']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 84, in main
    mse, global_sample_id = trainer.test(test_sample)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/utils/trainer.py", line 223, in test
    output = self.model(model_input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/graph_cast_net.py", line 219, in forward
    grid_node_feats_decoded = self.checkpoint_filter(partial(
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/graph_cast_net.py", line 253, in checkpoint_filter
    return partial_function()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/encoder_decoder.py", line 128, in forward
    edge_feature = self.edge_mlp(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/mlp.py", line 141, in forward
    mlp_sum = sum_efeat(
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/models/components/utils.py", line 106, in sum_efeat
    src_idx, dst_idx = graph.edges()
  File "/usr/local/lib/python3.10/dist-packages/dgl/view.py", line 179, in __call__
    return self._graph.all_edges(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/dgl/heterograph.py", line 3591, in all_edges
    src, dst, eid = self._graph.edges(self.get_etype_id(etype), order)
  File "/usr/local/lib/python3.10/dist-packages/dgl/heterograph_index.py", line 696, in edges
    edge_array = _CAPI_DGLHeteroEdges(self, int(etype), order)
  File "dgl/_ffi/_cython/./function.pxi", line 295, in dgl._ffi._cy3.core.FunctionBase.__call__
  File "dgl/_ffi/_cython/./function.pxi", line 227, in dgl._ffi._cy3.core.FuncCall
  File "dgl/_ffi/_cython/./function.pxi", line 217, in dgl._ffi._cy3.core.FuncCall3
dgl._ffi.base.DGLError: [21:16:18] /workspace/ktangsali/dgl/src/runtime/cuda/cuda_device_api.cc:117: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory
Stack trace:
  [bt] (0) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x58) [0x40010d381418]
  [bt] (1) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DGLContext, unsigned long, unsigned long, DGLDataType)+0x220) [0x40010de2fb64]
  [bt] (2) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DGLDataType, DGLContext)+0xd8) [0x40010da0eac4]
  [bt] (3) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::NewIdArray(long, DGLContext, unsigned char)+0x98) [0x40010d34d8c0]
  [bt] (4) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::runtime::NDArray dgl::aten::impl::Range<(DGLDeviceType)2, int>(int, int, DGLContext)+0xc8) [0x40010de6ce4c]
  [bt] (5) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::aten::Range(long, long, unsigned char, DGLContext)+0x168) [0x40010d34dc58]
  [bt] (6) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::UnitGraph::COO::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x1d8) [0x40010dd9c818]
  [bt] (7) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::UnitGraph::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x1c4) [0x40010dd8e298]
  [bt] (8) /usr/local/lib/python3.10/dist-packages/dgl/libdgl.so(dgl::HeteroGraph::Edges(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const+0x78) [0x40010db2221c]



Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-03-22 21:16:18,909][trainer][INFO] - [94mIteration    50 | Test MSE:   1.16 | Time taken:  7.40/ 1.33/ 8.73 sec | GPU memory: 82.1 GB | Global sample ID: 2108[0m
[rank2]:[E322 21:26:18.280456611 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
[rank2]:[E322 21:26:18.282420396 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E322 21:26:18.282427116 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 2] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E322 21:26:18.282433708 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E322 21:26:19.785001043 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.
[rank3]:[E322 21:26:19.785069393 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank3]:[E322 21:26:19.785073937 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 3] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank3]:[E322 21:26:19.785078737 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 21:26:19.848433306 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600080 milliseconds before timing out.
[rank0]:[E322 21:26:19.848489433 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E322 21:26:19.848494713 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E322 21:26:19.848497688 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 21:26:19.848533111 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=202, OpType=ALLREDUCE, NumelIn=13139968, NumelOut=13139968, Timeout(ms)=600000) ran for 600067 milliseconds before timing out.
[rank0]:[E322 21:26:19.848541367 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E322 21:26:19.848543575 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E322 21:26:19.848545591 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 21:26:19.948612664 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=203, OpType=ALLREDUCE, NumelIn=13398528, NumelOut=13398528, Timeout(ms)=600000) ran for 600078 milliseconds before timing out.
[rank0]:[E322 21:26:19.948620408 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E322 21:26:19.948623128 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E322 21:26:19.948625079 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 21:26:19.948631191 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=204, OpType=ALLREDUCE, NumelIn=8172544, NumelOut=8172544, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
[rank0]:[E322 21:26:19.948637271 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E322 21:26:19.948639479 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E322 21:26:19.948641399 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E322 21:26:19.609535833 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600022 milliseconds before timing out.
[rank1]:[E322 21:26:19.609595767 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank1]:[E322 21:26:19.609602391 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank1]:[E322 21:26:19.609605111 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E322 21:38:37.837099479 ProcessGroupNCCL.cpp:1496] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
[rank1]:[E322 21:38:37.850590365 ProcessGroupNCCL.cpp:1496] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
slurmstepd: error: *** STEP 293508.0 ON nid005021 CANCELLED AT 2025-03-22T21:43:50 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 293508 ON nid005021 CANCELLED AT 2025-03-22T21:43:50 DUE TO TIME LIMIT ***
