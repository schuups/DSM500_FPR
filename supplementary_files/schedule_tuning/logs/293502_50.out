wandb: Starting wandb agent 🕵️
2025-03-23 00:02:40,344 - wandb.wandb_agent - INFO - Running runs: []
2025-03-23 00:02:40,770 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-23 00:02:40,770 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 21
	schedule.phase1.iterations: 1000
	schedule.phase1.lr_end: 0.01
	schedule.phase1.lr_start: 0.001
	schedule.phase2.iterations: 200
	schedule.phase2.lr_objective: 0.005
	schedule.phase3.iterations: 200
	schedule.phase3.lr: 0.001
	schedule.phase3.rollout_steps_increments: 3
2025-03-23 00:02:40,773 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=21 schedule.phase1.iterations=1000 schedule.phase1.lr_end=0.01 schedule.phase1.lr_start=0.001 schedule.phase2.iterations=200 schedule.phase2.lr_objective=0.005 schedule.phase3.iterations=200 schedule.phase3.lr=0.001 schedule.phase3.rollout_steps_increments=3
2025-03-23 00:02:45,782 - wandb.wandb_agent - INFO - Running runs: ['ao0xjyom']
[2025-03-23 00:03:10,364][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-23 00:03:10,364][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-23 00:03:10,364][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-23 00:03:10,371][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250323_000311-ao0xjyom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_293809_25/03/23_00:03:10_RUN_01
wandb: ⭐️ View project at https://wandb.ai/schups/DSM500_FPR
wandb: 🧹 View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/mpgn79e1
wandb: 🚀 View run at https://wandb.ai/schups/DSM500_FPR/runs/ao0xjyom
[2025-03-23 00:03:12,543][trainer][INFO] - [94mSetting seed to 21[0m
[2025-03-23 00:03:12,594][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-23 00:03:12,598][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-23 00:03:12,599][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-23 00:03:12,600][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-23 00:03:16,960][trainer][INFO] - [94mModel created. Trainable parameters count is 35'248'149[0m
[2025-03-23 00:03:22,657][trainer][INFO] - [92mLoaded train datapipe of size 53'947 samples[0m
[2025-03-23 00:03:24,491][trainer][INFO] - [92mLoaded test datapipe of size 2'903 samples[0m
[2025-03-23 00:03:24,558][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-23 00:03:24,640][main][INFO] - [94mTraining started...[0m
[2025-03-23 00:04:19,755][trainer][INFO] - Iteration     1 | Train loss: 4.87 | Time taken:  9.04/46.06/55.10 sec | GPU memory: 79.1 GB | Global sample ID: 3374
[2025-03-23 00:04:20,178][trainer][INFO] - Iteration     2 | Train loss: 4.97 | Time taken:  0.00/ 0.27/ 0.27 sec | GPU memory: 82.1 GB | Global sample ID: 30756
[2025-03-23 00:04:20,567][trainer][INFO] - Iteration     3 | Train loss: 4.82 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 1692
[2025-03-23 00:04:21,411][trainer][INFO] - Iteration     4 | Train loss: 4.98 | Time taken:  0.00/ 0.30/ 0.31 sec | GPU memory: 82.1 GB | Global sample ID: 14842
[2025-03-23 00:04:21,851][trainer][INFO] - Iteration     5 | Train loss: 4.85 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 41459
[2025-03-23 00:04:22,240][trainer][INFO] - Iteration     6 | Train loss: 4.82 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 22876
[2025-03-23 00:04:22,630][trainer][INFO] - Iteration     7 | Train loss: 4.84 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 36064
[2025-03-23 00:04:23,183][trainer][INFO] - Iteration     8 | Train loss: 4.71 | Time taken:  0.00/ 0.44/ 0.44 sec | GPU memory: 82.1 GB | Global sample ID: 37044
[2025-03-23 00:04:23,706][trainer][INFO] - Iteration     9 | Train loss: 5.08 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40746
[2025-03-23 00:04:24,094][trainer][INFO] - Iteration    10 | Train loss: 4.90 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 33084
[2025-03-23 00:04:24,484][trainer][INFO] - Iteration    11 | Train loss: 4.75 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 45618
[2025-03-23 00:04:24,874][trainer][INFO] - Iteration    12 | Train loss: 4.80 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 30078
[2025-03-23 00:04:25,276][trainer][INFO] - Iteration    13 | Train loss: 5.12 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 5703
[2025-03-23 00:04:25,713][trainer][INFO] - Iteration    14 | Train loss: 4.83 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15297
[2025-03-23 00:04:26,099][trainer][INFO] - Iteration    15 | Train loss: 5.11 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 4167
[2025-03-23 00:04:26,565][trainer][INFO] - Iteration    16 | Train loss: 4.81 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 16140
[2025-03-23 00:04:26,979][trainer][INFO] - Iteration    17 | Train loss: 4.90 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 38984
[2025-03-23 00:04:27,371][trainer][INFO] - Iteration    18 | Train loss: 5.02 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 48331
[2025-03-23 00:04:27,758][trainer][INFO] - Iteration    19 | Train loss: 4.99 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 34640
[2025-03-23 00:04:28,212][trainer][INFO] - Iteration    20 | Train loss: 4.94 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 3219
[2025-03-23 00:04:28,613][trainer][INFO] - Iteration    21 | Train loss: 4.78 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15494
[2025-03-23 00:04:29,004][trainer][INFO] - Iteration    22 | Train loss: 4.86 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 49159
[2025-03-23 00:04:29,392][trainer][INFO] - Iteration    23 | Train loss: 4.76 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40366
[2025-03-23 00:04:29,784][trainer][INFO] - Iteration    24 | Train loss: 4.82 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2423
[2025-03-23 00:04:30,174][trainer][INFO] - Iteration    25 | Train loss: 4.90 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 3703
[2025-03-23 00:04:30,622][trainer][INFO] - Iteration    26 | Train loss: 5.06 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 32159
[2025-03-23 00:04:31,012][trainer][INFO] - Iteration    27 | Train loss: 5.04 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 40868
[2025-03-23 00:04:31,525][trainer][INFO] - Iteration    28 | Train loss: 4.83 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 26244
[2025-03-23 00:04:31,911][trainer][INFO] - Iteration    29 | Train loss: 4.78 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 3851
[2025-03-23 00:04:32,384][trainer][INFO] - Iteration    30 | Train loss: 4.75 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 31380
[2025-03-23 00:04:32,770][trainer][INFO] - Iteration    31 | Train loss: 5.16 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 45111
[2025-03-23 00:04:33,166][trainer][INFO] - Iteration    32 | Train loss: 4.74 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 48735
[2025-03-23 00:04:33,553][trainer][INFO] - Iteration    33 | Train loss: 4.95 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 52716
[2025-03-23 00:04:33,941][trainer][INFO] - Iteration    34 | Train loss: 4.65 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 35442
[2025-03-23 00:04:34,331][trainer][INFO] - Iteration    35 | Train loss: 4.87 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 28555
[2025-03-23 00:04:34,732][trainer][INFO] - Iteration    36 | Train loss: 4.93 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 52525
[2025-03-23 00:04:35,127][trainer][INFO] - Iteration    37 | Train loss: 4.66 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 53105
[2025-03-23 00:04:35,725][trainer][INFO] - Iteration    38 | Train loss: 5.05 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 2872
[2025-03-23 00:04:36,114][trainer][INFO] - Iteration    39 | Train loss: 4.93 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 46733
[2025-03-23 00:04:36,502][trainer][INFO] - Iteration    40 | Train loss: 5.11 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 26205
[2025-03-23 00:04:36,894][trainer][INFO] - Iteration    41 | Train loss: 5.03 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 50900
[2025-03-23 00:04:37,284][trainer][INFO] - Iteration    42 | Train loss: 4.91 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 40652
[2025-03-23 00:04:37,673][trainer][INFO] - Iteration    43 | Train loss: 4.76 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 20630
[2025-03-23 00:04:38,062][trainer][INFO] - Iteration    44 | Train loss: 4.73 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 43052
[2025-03-23 00:04:38,457][trainer][INFO] - Iteration    45 | Train loss: 4.70 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 5155
[2025-03-23 00:04:38,852][trainer][INFO] - Iteration    46 | Train loss: 4.97 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 28846
[2025-03-23 00:04:39,242][trainer][INFO] - Iteration    47 | Train loss: 4.85 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 43929
[2025-03-23 00:04:39,635][trainer][INFO] - Iteration    48 | Train loss: 4.84 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 12607
[2025-03-23 00:04:40,025][trainer][INFO] - Iteration    49 | Train loss: 4.93 | Time taken:  0.00/ 0.28/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 30861
[2025-03-23 00:04:40,414][trainer][INFO] - Iteration    50 | Train loss: 4.78 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 45256
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=21', 'schedule.phase1.iterations=1000', 'schedule.phase1.lr_end=0.01', 'schedule.phase1.lr_start=0.001', 'schedule.phase2.iterations=200', 'schedule.phase2.lr_objective=0.005', 'schedule.phase3.iterations=200', 'schedule.phase3.lr=0.001', 'schedule.phase3.rollout_steps_increments=3']
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=21', 'schedule.phase1.iterations=1000', 'schedule.phase1.lr_end=0.01', 'schedule.phase1.lr_start=0.001', 'schedule.phase2.iterations=200', 'schedule.phase2.lr_objective=0.005', 'schedule.phase3.iterations=200', 'schedule.phase3.lr=0.001', 'schedule.phase3.rollout_steps_increments=3']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 1158676480 bytes on device 3.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 1158676480 bytes on device 2.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-03-23 00:04:50,793][trainer][INFO] - [94mIteration    50 | Test MSE:   1.23 | Time taken:  8.97/ 1.39/10.36 sec | GPU memory: 82.1 GB | Global sample ID: 2108[0m
[rank3]:[E323 00:14:49.720193309 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
[rank3]:[E323 00:14:49.722764708 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank3]:[E323 00:14:49.722772259 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 3] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank3]:[E323 00:14:49.722778339 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E323 00:14:49.822517542 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600079 milliseconds before timing out.
[rank2]:[E323 00:14:49.822577348 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E323 00:14:49.822582820 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 2] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E323 00:14:49.822586756 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E323 00:14:51.133491389 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
[rank1]:[E323 00:14:51.133553211 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank1]:[E323 00:14:51.133557371 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank1]:[E323 00:14:51.133560251 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 00:14:51.135171843 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600059 milliseconds before timing out.
[rank0]:[E323 00:14:51.135241249 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E323 00:14:51.135246625 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E323 00:14:51.135249825 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 00:14:51.135288287 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=202, OpType=ALLREDUCE, NumelIn=13139968, NumelOut=13139968, Timeout(ms)=600000) ran for 600047 milliseconds before timing out.
[rank0]:[E323 00:14:51.135295647 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E323 00:14:51.135297663 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E323 00:14:51.135299711 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 00:14:51.235368951 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=203, OpType=ALLREDUCE, NumelIn=13398528, NumelOut=13398528, Timeout(ms)=600000) ran for 600058 milliseconds before timing out.
[rank0]:[E323 00:14:51.235381111 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E323 00:14:51.235383863 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E323 00:14:51.235386070 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E323 00:14:51.235391990 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=204, OpType=ALLREDUCE, NumelIn=8172544, NumelOut=8172544, Timeout(ms)=600000) ran for 600025 milliseconds before timing out.
[rank0]:[E323 00:14:51.235397174 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E323 00:14:51.235399190 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E323 00:14:51.235401046 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E323 00:27:06.612585280 ProcessGroupNCCL.cpp:1496] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
slurmstepd: error: *** STEP 293809.0 ON nid005017 CANCELLED AT 2025-03-23T00:32:06 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 293809 ON nid005017 CANCELLED AT 2025-03-23T00:32:06 DUE TO TIME LIMIT ***
