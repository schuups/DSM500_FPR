wandb: Starting wandb agent ðŸ•µï¸
2025-03-22 22:28:07,784 - wandb.wandb_agent - INFO - Running runs: []
2025-03-22 22:28:08,466 - wandb.wandb_agent - INFO - Agent received command: run
2025-03-22 22:28:08,466 - wandb.wandb_agent - INFO - Agent starting run with config:
	checkpoint.enabled: False
	datapipe.seed: 42
	schedule.phase1.iterations: 2000
	schedule.phase1.lr_end: 1
	schedule.phase1.lr_start: 0.001
	schedule.phase2.iterations: 100
	schedule.phase2.lr_objective: 0.0001
	schedule.phase3.iterations: 500
	schedule.phase3.lr: 0.0001
	schedule.phase3.rollout_steps_increments: 4
2025-03-22 22:28:08,469 - wandb.wandb_agent - INFO - About to run command: python -m torch.distributed.run --nnodes=1 --nproc_per_node=4 train_graphcast.py checkpoint.enabled=False datapipe.seed=42 schedule.phase1.iterations=2000 schedule.phase1.lr_end=1 schedule.phase1.lr_start=0.001 schedule.phase2.iterations=100 schedule.phase2.lr_objective=0.0001 schedule.phase3.iterations=500 schedule.phase3.lr=0.0001 schedule.phase3.rollout_steps_increments=4
2025-03-22 22:28:13,477 - wandb.wandb_agent - INFO - Running runs: ['yvmm34p5']
[2025-03-22 22:28:33,158][main][INFO] - [94mRank: 2, Device: cuda:2[0m
[2025-03-22 22:28:33,158][main][INFO] - [94mRank: 3, Device: cuda:3[0m
[2025-03-22 22:28:33,158][main][INFO] - [94mRank: 1, Device: cuda:1[0m
[2025-03-22 22:28:33,161][main][INFO] - [94mRank: 0, Device: cuda:0[0m
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Ignoring project 'DSM500_FPR' when running a sweep.
wandb: WARNING Ignoring entity 'schups' when running a sweep.
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/wandb/wandb/run-20250322_222833-yvmm34p5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Baseline_64377_25/03/22_22:28:33_RUN_01
wandb: â­ï¸ View project at https://wandb.ai/schups/DSM500_FPR
wandb: ðŸ§¹ View sweep at https://wandb.ai/schups/DSM500_FPR/sweeps/mpgn79e1
wandb: ðŸš€ View run at https://wandb.ai/schups/DSM500_FPR/runs/yvmm34p5
[2025-03-22 22:28:35,421][trainer][INFO] - [94mSetting seed to 42[0m
[2025-03-22 22:28:35,471][cache][INFO] - [94mLoading cache for 'meshes'.[0m
[2025-03-22 22:28:35,471][cache][INFO] - [94mChecking if 'meshes' is cached.[0m
[2025-03-22 22:28:35,472][cache][INFO] - [94m-> HIT! '/iopsstor/scratch/cscs/stefschu/DSM500_FPR/cache/icosahedron_meshes.pickled' exists.[0m
[2025-03-22 22:28:35,473][cache][INFO] - [94m-> Checking guard 'MeshesCacheGuard'.[0m
[2025-03-22 22:28:39,867][trainer][INFO] - [94mModel created. Trainable parameters count is 35'248'149[0m
[2025-03-22 22:28:45,674][trainer][INFO] - [92mLoaded train datapipe of size 53'947 samples[0m
[2025-03-22 22:28:47,430][trainer][INFO] - [92mLoaded test datapipe of size 2'903 samples[0m
[2025-03-22 22:28:47,433][main][INFO] - [94mInitializing dataloaders...[0m
[2025-03-22 22:28:47,433][main][INFO] - [94mTraining started...[0m
[2025-03-22 22:29:36,490][trainer][INFO] - Iteration     1 | Train loss: 3.98 | Time taken:  7.75/41.30/49.06 sec | GPU memory: 79.1 GB | Global sample ID: 35335
[2025-03-22 22:29:36,889][trainer][INFO] - Iteration     2 | Train loss: 3.73 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 39950
[2025-03-22 22:29:37,284][trainer][INFO] - Iteration     3 | Train loss: 3.60 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 17002
[2025-03-22 22:29:37,871][trainer][INFO] - Iteration     4 | Train loss: 3.71 | Time taken:  0.00/ 0.32/ 0.32 sec | GPU memory: 82.1 GB | Global sample ID: 19918
[2025-03-22 22:29:38,265][trainer][INFO] - Iteration     5 | Train loss: 3.87 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 15961
[2025-03-22 22:29:38,704][trainer][INFO] - Iteration     6 | Train loss: 4.01 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 11951
[2025-03-22 22:29:39,101][trainer][INFO] - Iteration     7 | Train loss: 4.09 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 13377
[2025-03-22 22:29:39,646][trainer][INFO] - Iteration     8 | Train loss: 3.81 | Time taken:  0.00/ 0.44/ 0.44 sec | GPU memory: 82.1 GB | Global sample ID: 25632
[2025-03-22 22:29:40,042][trainer][INFO] - Iteration     9 | Train loss: 4.02 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 18934
[2025-03-22 22:29:40,439][trainer][INFO] - Iteration    10 | Train loss: 3.94 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 15906
[2025-03-22 22:29:40,835][trainer][INFO] - Iteration    11 | Train loss: 3.89 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 46823
[2025-03-22 22:29:41,235][trainer][INFO] - Iteration    12 | Train loss: 3.83 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 15819
[2025-03-22 22:29:41,633][trainer][INFO] - Iteration    13 | Train loss: 3.61 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 48235
[2025-03-22 22:29:42,036][trainer][INFO] - Iteration    14 | Train loss: 3.76 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 16177
[2025-03-22 22:29:42,432][trainer][INFO] - Iteration    15 | Train loss: 3.73 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9930
[2025-03-22 22:29:42,828][trainer][INFO] - Iteration    16 | Train loss: 3.05 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 41644
[2025-03-22 22:29:43,227][trainer][INFO] - Iteration    17 | Train loss: 3.05 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3496
[2025-03-22 22:29:43,624][trainer][INFO] - Iteration    18 | Train loss: 3.16 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 5381
[2025-03-22 22:29:44,022][trainer][INFO] - Iteration    19 | Train loss: 2.75 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 14034
[2025-03-22 22:29:44,420][trainer][INFO] - Iteration    20 | Train loss: 2.67 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 13919
[2025-03-22 22:29:45,002][trainer][INFO] - Iteration    21 | Train loss: 2.53 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 50601
[2025-03-22 22:29:45,398][trainer][INFO] - Iteration    22 | Train loss: 2.51 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3080
[2025-03-22 22:29:45,842][trainer][INFO] - Iteration    23 | Train loss: 2.41 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 37483
[2025-03-22 22:29:46,241][trainer][INFO] - Iteration    24 | Train loss: 2.46 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 20487
[2025-03-22 22:29:46,642][trainer][INFO] - Iteration    25 | Train loss: 2.07 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 50335
[2025-03-22 22:29:47,034][trainer][INFO] - Iteration    26 | Train loss: 2.05 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 33107
[2025-03-22 22:29:47,431][trainer][INFO] - Iteration    27 | Train loss: 1.84 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 8050
[2025-03-22 22:29:47,832][trainer][INFO] - Iteration    28 | Train loss: 1.78 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 6767
[2025-03-22 22:29:48,229][trainer][INFO] - Iteration    29 | Train loss: 1.78 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 17381
[2025-03-22 22:29:48,642][trainer][INFO] - Iteration    30 | Train loss: 1.62 | Time taken:  0.00/ 0.28/ 0.28 sec | GPU memory: 82.1 GB | Global sample ID: 18720
[2025-03-22 22:29:49,040][trainer][INFO] - Iteration    31 | Train loss: 1.37 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 3462
[2025-03-22 22:29:49,438][trainer][INFO] - Iteration    32 | Train loss: 1.28 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 43268
[2025-03-22 22:29:49,837][trainer][INFO] - Iteration    33 | Train loss: 1.15 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 10175
[2025-03-22 22:29:50,242][trainer][INFO] - Iteration    34 | Train loss: 0.90 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 15632
[2025-03-22 22:29:50,650][trainer][INFO] - Iteration    35 | Train loss: 0.79 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 25503
[2025-03-22 22:29:51,043][trainer][INFO] - Iteration    36 | Train loss: 0.70 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 1375
[2025-03-22 22:29:51,441][trainer][INFO] - Iteration    37 | Train loss: 0.52 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 17910
[2025-03-22 22:29:51,840][trainer][INFO] - Iteration    38 | Train loss: 0.45 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 33040
[2025-03-22 22:29:52,238][trainer][INFO] - Iteration    39 | Train loss: 0.41 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 44623
[2025-03-22 22:29:52,639][trainer][INFO] - Iteration    40 | Train loss: 0.35 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 38909
[2025-03-22 22:29:53,038][trainer][INFO] - Iteration    41 | Train loss: 0.36 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 37918
[2025-03-22 22:29:53,439][trainer][INFO] - Iteration    42 | Train loss: 0.38 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9131
[2025-03-22 22:29:53,842][trainer][INFO] - Iteration    43 | Train loss: 0.30 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 6797
[2025-03-22 22:29:54,240][trainer][INFO] - Iteration    44 | Train loss: 0.30 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 9661
[2025-03-22 22:29:54,638][trainer][INFO] - Iteration    45 | Train loss: 0.28 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 38496
[2025-03-22 22:29:55,037][trainer][INFO] - Iteration    46 | Train loss: 0.27 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 38616
[2025-03-22 22:29:55,436][trainer][INFO] - Iteration    47 | Train loss: 0.28 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 30779
[2025-03-22 22:29:55,847][trainer][INFO] - Iteration    48 | Train loss: 0.26 | Time taken:  0.00/ 0.30/ 0.30 sec | GPU memory: 82.1 GB | Global sample ID: 20689
[2025-03-22 22:29:56,244][trainer][INFO] - Iteration    49 | Train loss: 0.24 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 47508
[2025-03-22 22:29:56,644][trainer][INFO] - Iteration    50 | Train loss: 0.24 | Time taken:  0.00/ 0.29/ 0.29 sec | GPU memory: 82.1 GB | Global sample ID: 14450
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=42', 'schedule.phase1.iterations=2000', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.001', 'schedule.phase2.iterations=100', 'schedule.phase2.lr_objective=0.0001', 'schedule.phase3.iterations=500', 'schedule.phase3.lr=0.0001', 'schedule.phase3.rollout_steps_increments=4']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 1158676480 bytes on device 2.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['checkpoint.enabled=False', 'datapipe.seed=42', 'schedule.phase1.iterations=2000', 'schedule.phase1.lr_end=1', 'schedule.phase1.lr_start=0.001', 'schedule.phase2.iterations=100', 'schedule.phase2.lr_objective=0.0001', 'schedule.phase3.iterations=500', 'schedule.phase3.lr=0.0001', 'schedule.phase3.rollout_steps_increments=4']
Traceback (most recent call last):
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/train_graphcast.py", line 77, in main
    test_sample = next(iterator_testing)
  File "/iopsstor/scratch/cscs/stefschu/DSM500_FPR/modulus-baseline/modulus/datapipes/era5_hdf5.py", line 250, in __iter__
    _pipeline = dali_pth.DALIGenericIterator([self.pipe], [
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 224, in __init__
    self._first_batch = DALIGenericIterator.__next__(self)
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/pytorch/__init__.py", line 239, in __next__
    outputs = self._get_outputs()
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/plugin/base_iterator.py", line 385, in _get_outputs
    outputs.append(p.share_outputs())
  File "/usr/local/lib/python3.10/dist-packages/nvidia/dali/pipeline.py", line 1215, in share_outputs
    return self._pipe.ShareOutputs()
RuntimeError: Critical error in pipeline:
Error in MIXED operator `nvidia.dali.ops.MakeContiguous` encountered:

Can't allocate 1158676480 bytes on device 1.
Current pipeline object is no longer valid.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-03-22 22:30:05,820][trainer][INFO] - [94mIteration    50 | Test MSE:   1.02 | Time taken:  7.85/ 1.33/ 9.17 sec | GPU memory: 82.1 GB | Global sample ID: 1771[0m
[rank2]:[E322 22:40:04.576649646 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
[rank2]:[E322 22:40:04.578329555 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E322 22:40:04.578337267 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 2] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank2]:[E322 22:40:04.578343474 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E322 22:40:04.785340852 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600011 milliseconds before timing out.
[rank1]:[E322 22:40:04.785400754 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank1]:[E322 22:40:04.785405618 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 201, last enqueued NCCL work: 201, last completed NCCL work: 200.
[rank1]:[E322 22:40:04.785409842 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E322 22:40:05.986147300 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
[rank3]:[E322 22:40:05.986213761 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank3]:[E322 22:40:05.986218625 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 3] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank3]:[E322 22:40:05.986222017 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 22:40:06.278390871 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=201, OpType=ALLREDUCE, NumelIn=537109, NumelOut=537109, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank0]:[E322 22:40:06.278550034 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E322 22:40:06.278560305 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 201, last enqueued NCCL work: 204, last completed NCCL work: 200.
[rank0]:[E322 22:40:06.278565265 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 22:40:06.278644878 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=202, OpType=ALLREDUCE, NumelIn=13139968, NumelOut=13139968, Timeout(ms)=600000) ran for 600007 milliseconds before timing out.
[rank0]:[E322 22:40:06.278657902 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E322 22:40:06.278660910 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 202, last enqueued NCCL work: 204, last completed NCCL work: 201.
[rank0]:[E322 22:40:06.278664014 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 22:40:06.378709624 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=203, OpType=ALLREDUCE, NumelIn=13398528, NumelOut=13398528, Timeout(ms)=600000) ran for 600010 milliseconds before timing out.
[rank0]:[E322 22:40:06.378723415 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E322 22:40:06.378727127 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 203, last enqueued NCCL work: 204, last completed NCCL work: 202.
[rank0]:[E322 22:40:06.378730615 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E322 22:40:06.478806112 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=204, OpType=ALLREDUCE, NumelIn=8172544, NumelOut=8172544, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
[rank0]:[E322 22:40:06.478817567 ProcessGroupNCCL.cpp:1795] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E322 22:40:06.478820543 ProcessGroupNCCL.cpp:1844] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 204, last enqueued NCCL work: 204, last completed NCCL work: 203.
[rank0]:[E322 22:40:06.478825119 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E322 22:52:30.232907843 ProcessGroupNCCL.cpp:1496] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL's watchdog got stuck for 480 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API (e.g., CudaEventDestroy) hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api (for example, CudaEventDestroy), or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
slurmstepd: error: *** STEP 64377.0 ON nid007209 CANCELLED AT 2025-03-22T22:57:30 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 64377 ON nid007209 CANCELLED AT 2025-03-22T22:57:30 DUE TO TIME LIMIT ***
