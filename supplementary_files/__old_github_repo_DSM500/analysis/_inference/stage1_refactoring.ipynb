{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/modulus/distributed/manager.py:329: UserWarning: Distributed manager is already intialized\n",
      "  warn(\"Distributed manager is already intialized\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import hydra\n",
    "import h5py\n",
    "import json\n",
    "from omegaconf import DictConfig\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from hydra.utils import to_absolute_path\n",
    "from modulus.distributed import DistributedManager\n",
    "\n",
    "from modulus.launch.logging import PythonLogger\n",
    "from modulus.datapipes.climate.era5_hdf5_newest import fix_latitude_alignment\n",
    "\n",
    "# FIXIT: Move it under modulus\n",
    "from inference import Inference\n",
    "\n",
    "import os\n",
    "os.environ[\"MODULUS_DISTRIBUTED_INITIALIZATION_METHOD\"] = \"ENV\"\n",
    "os.environ[\"SLURM_PROCID\"] = os.environ[\"RANK\"] = os.environ[\"SLURM_LOCALID\"] = \"0\"\n",
    "os.environ[\"SLURM_NTASKS\"] = os.environ[\"SLURM_NPROCS\"] = os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"23456\"\n",
    "\n",
    "import hydra\n",
    "with hydra.initialize(config_path=\"..\", version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=\"config_new\")\n",
    "\n",
    "logger = PythonLogger(\"main\")\n",
    "logger.file_logging()\n",
    "DistributedManager.initialize()\n",
    "\n",
    "inference = Inference(cfg, logger)\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, inference, cfg):\n",
    "        self.inference = inference\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.model, self.datapipe = inference.load_model_and_datapipe()\n",
    "\n",
    "        # Data containers\n",
    "        self.initial_conditions = None\n",
    "        self.outputs = None\n",
    "        self.reanalysis = None\n",
    "        self.generated_channels = None\n",
    "        self.forecasts = None\n",
    "        self.global_sample_ids = None\n",
    "        self.timestamps = None\n",
    "\n",
    "        # Climatology\n",
    "        self.climatology = None\n",
    "\n",
    "        # Metrics\n",
    "        self.mse = None\n",
    "\n",
    "        # Flags\n",
    "        self.samples_loaded = False\n",
    "        self.forecasts_computed = False\n",
    "\n",
    "    def load_samples(self):\n",
    "        assert self.samples_loaded is False, \"Samples already loaded\"\n",
    "\n",
    "        for sample_i, data in enumerate(self.datapipe):\n",
    "            # dict_keys(['epoch_idx', 'idx_in_epoch', 'global_sample_id', 'output', 'timestamps', 'input'])\n",
    "            # data['input'].shape, data[\"output\"].shape, data[\"global_sample_id\"].shape, data[\"timestamps\"].shape\n",
    "            # (torch.Size([1, 1, 31, 721, 1440]),\n",
    "            #  torch.Size([1, 24, 31, 721, 1440]),\n",
    "            #  torch.Size([1]),\n",
    "            #  torch.Size([1, 25]))\n",
    "            # type(data['input']), type(data[\"output\"]), type(data[\"global_sample_id\"]), type(data[\"timestamps\"])\n",
    "            # (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "            data = data[0]\n",
    "\n",
    "            if self.initial_conditions is None:\n",
    "                self.initial_conditions = data['input']\n",
    "                self.outputs = data[\"output\"]\n",
    "                self.global_sample_ids = data[\"global_sample_id\"]\n",
    "                self.timestamps = data[\"timestamps\"]\n",
    "            else:\n",
    "                self.initial_conditions = torch.concatenate([self.initial_conditions, data['input']], dim=0)\n",
    "                self.outputs = torch.concatenate([self.outputs, data[\"output\"]], dim=0)\n",
    "                self.global_sample_ids = torch.concatenate([self.global_sample_ids, data[\"global_sample_id\"]], dim=0)\n",
    "                self.timestamps = torch.concatenate([self.timestamps, data[\"timestamps\"]], dim=0)\n",
    "\n",
    "            if self.initial_conditions.shape[0] == self.cfg.inference.samples:\n",
    "                break\n",
    "        \n",
    "        # torch.Size([3, 24, 21, 721, 1440]), torch.Size([3, 24, 10, 721, 1440]) <- outputs torch.Size([3, 24, 31, 721, 1440])\n",
    "        channels_count_reanalysis = self.cfg.output_channels # supposedly 21\n",
    "        channels_count_generated = self.outputs.shape[2] - channels_count_reanalysis # supposedly 10, if all options enabled\n",
    "        self.reanalysis, self.generated_channels = torch.split(self.outputs, [channels_count_reanalysis, channels_count_generated], dim=2)\n",
    "        self.outputs = None\n",
    "        self.forecasts = torch.zeros(self.reanalysis.shape)\n",
    "\n",
    "        # cast and move as needed\n",
    "        self.initial_conditions = self.initial_conditions.to(inference.dtype).to(self.inference.device)\n",
    "        self.reanalysis = self.reanalysis.to(inference.dtype).to(self.inference.device)\n",
    "        self.generated_channels = self.generated_channels.to(inference.dtype).to(self.inference.device)\n",
    "        self.forecasts = self.forecasts.to(inference.dtype).to(self.inference.device)\n",
    "\n",
    "        # Load climatology for samples\n",
    "        self.climatology = torch.zeros(self.reanalysis.shape, device=\"cpu\")\n",
    "        with h5py.File(Path(cfg.dataset.base_path) / \"climatology.h5\", \"r\") as f:\n",
    "            for sample_i, global_sample_id in enumerate(self.global_sample_ids):\n",
    "                idx_start = global_sample_id + 1\n",
    "                idx_end = idx_start + self.cfg.inference.rollout_steps\n",
    "                self.climatology[sample_i, :] = torch.tensor(f[\"climatology\"][idx_start:idx_end])\n",
    "        self.climatology = fix_latitude_alignment(self.climatology)\n",
    "\n",
    "        self.samples_loaded = True\n",
    "\n",
    "    def compute_forecasts(self):\n",
    "        assert self.samples_loaded is True, \"Samples not loaded\"\n",
    "        assert self.forecasts_computed is False, \"Forecasts already computed\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sample_i in range(self.cfg.inference.samples):\n",
    "                for step in range(self.cfg.inference.rollout_steps + 1):\n",
    "                    # Step 0 is the initial condition\n",
    "                    if step == 0:\n",
    "                        next_input = self.initial_conditions[sample_i].unsqueeze(0)\n",
    "                    else:\n",
    "                        self.forecasts[sample_i, step - 1] = self.model(next_input)\n",
    "                        next_input = torch.concatenate([\n",
    "                            self.forecasts[sample_i, step - 1],\n",
    "                            self.generated_channels[sample_i, step - 1]\n",
    "                        ], dim=0).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Now move everything off the GPU\n",
    "        self.initial_conditions = self.initial_conditions.to(torch.float32).cpu()\n",
    "        self.reanalysis = self.reanalysis.to(torch.float32).cpu()\n",
    "        self.generated_channels = self.generated_channels.to(torch.float32).cpu()\n",
    "        self.forecasts = self.forecasts.to(torch.float32).cpu()\n",
    "\n",
    "        self.forecasts_computed = True\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        assert self.forecasts_computed, \"Forecasts not computed\"\n",
    "\n",
    "        # MSE\n",
    "        self.mse = torch.square(self.reanalysis - self.forecasts).mean(dim=(-2, -1))\n",
    "        \n",
    "        # ACC\n",
    "        forecast_anomalies = self.forecasts - self.climatology\n",
    "        reanalysis_anomalies = self.reanalysis - self.climatology\n",
    "        forecast_anomalies_std = torch.sqrt(torch.sum(torch.square(forecast_anomalies), dim=(-2, -1)))\n",
    "        reanalysis_anomalies_std = torch.sqrt(torch.sum(torch.square(reanalysis_anomalies), dim=(-2, -1)))\n",
    "        numerator = torch.sum(forecast_anomalies * reanalysis_anomalies, dim=(-2, -1))\n",
    "        self.acc = numerator / (forecast_anomalies_std * reanalysis_anomalies_std)\n",
    "\n",
    "dg = DataGenerator(inference, cfg)\n",
    "dg.load_samples()\n",
    "dg.compute_forecasts()\n",
    "dg.compute_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9987, 0.9978, 0.9963, 0.9947, 0.9924, 0.9905])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.acc[0,:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inference import Inference\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_arm64",
   "language": "python",
   "name": "venv_arm64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
