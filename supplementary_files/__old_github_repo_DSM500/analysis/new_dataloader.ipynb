{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/modulus/distributed/manager.py:329: UserWarning: Distributed manager is already intialized\n",
      "  warn(\"Distributed manager is already intialized\")\n",
      "\u001b[93mProvided checkpoint directory /iopsstor/scratch/cscs/stefschu/DSM500/github/analysis/checkpoints does not exist, skipping load\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation datapipe of size 2912\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%run __common.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_broadcast_buffers',\n",
       " '_cuda',\n",
       " '_device',\n",
       " '_distributed',\n",
       " '_find_unused_parameters',\n",
       " '_group_names',\n",
       " '_group_ranks',\n",
       " '_groups',\n",
       " '_initialization_method',\n",
       " '_is_initialized',\n",
       " '_local_rank',\n",
       " '_rank',\n",
       " '_shared_state',\n",
       " '_world_size',\n",
       " 'broadcast_buffers',\n",
       " 'cleanup',\n",
       " 'create_group_from_node',\n",
       " 'create_groups_from_config',\n",
       " 'create_orthogonal_process_group',\n",
       " 'create_process_subgroup',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'distributed',\n",
       " 'find_unused_parameters',\n",
       " 'get_available_backend',\n",
       " 'group',\n",
       " 'group_name',\n",
       " 'group_names',\n",
       " 'group_rank',\n",
       " 'group_size',\n",
       " 'initialize',\n",
       " 'initialize_env',\n",
       " 'initialize_open_mpi',\n",
       " 'initialize_slurm',\n",
       " 'is_initialized',\n",
       " 'local_rank',\n",
       " 'rank',\n",
       " 'setup',\n",
       " 'world_size']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "from modulus.datapipes.climate.era5_hdf5_new import ERA5HDF5Datapipe\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ERA5HDF5Datapipe' object has no attribute 'statistics_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_memory\u001b[39m(tensor):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39melement_size() \u001b[38;5;241m*\u001b[39m tensor\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Convert to MB\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mERA5HDF5Datapipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatistics_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps_ahead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m     42\u001b[0m mp\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/modulus/datapipes/climate/era5_hdf5_new.py:37\u001b[0m, in \u001b[0;36mERA5HDF5Datapipe.__init__\u001b[0;34m(self, data_dir, static_data_dir, statistics_dir, num_steps_ahead, device, dtype)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatistics_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_static_data(path\u001b[38;5;241m=\u001b[39mstatic_data_dir)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_data_files()\n",
      "File \u001b[0;32m/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/modulus/datapipes/climate/era5_hdf5_new.py:55\u001b[0m, in \u001b[0;36mERA5HDF5Datapipe.load_statistics\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_statistics\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatistics_dir\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ERA5HDF5Datapipe' object has no attribute 'statistics_dir'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"HDF5_CHUNK_CACHE_SIZE\"] = str(100 * 1024 * 1024)  # 100MB\n",
    "os.environ[\"HDF5_SWMR_WRITE\"] = \"1\"\n",
    "os.environ[\"HDF5_SWMR_READ\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import gc\n",
    "from modulus.datapipes.climate.era5_hdf5_new import ERA5HDF5Datapipe\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def list_cuda_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                print(type(obj), obj.size(), obj.dtype)\n",
    "        except:\n",
    "            pass  # Skip any errors\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    cached = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"[{tag}] Allocated: {allocated:.2f} MB | Cached: {cached:.2f} MB\")\n",
    "\n",
    "def tensor_memory(tensor):\n",
    "    return tensor.element_size() * tensor.numel() / 1024**2  # Convert to MB\n",
    "\n",
    "train = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr\",\n",
    "    statistics_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    static_data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\",\n",
    "    num_steps_ahead=10,\n",
    ")\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "#mp.set_start_method(\"spawn\", force=True)\n",
    "train_loader = data.DataLoader(train, \n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "collector = []\n",
    "for i, (sample, stats) in enumerate(train_loader):\n",
    "    #sample = sample.to(\"cuda:0\").to(torch.bfloat16)\n",
    "\n",
    "    collector.append(stats)\n",
    "    worker_id = int(stats[\"worker_id\"])\n",
    "    year = int(stats[\"year\"])\n",
    "    time_total = float(stats[\"time_total\"])\n",
    "    time_probe = float(stats[\"time_probe\"])\n",
    "\n",
    "    print(f\"Worker {worker_id} | Year {year} | Time total {time_total:2f} s | Time probed {time_probe:.2f} s | {(time_probe * 100 / time_total):.2f} %\")\n",
    "    if i == 30:\n",
    "        break\n",
    "\n",
    "time_totals = list()\n",
    "time_probes = list()\n",
    "for c in collector:\n",
    "    time_totals.append(float(c[\"time_total\"]))\n",
    "    time_probes.append(float(c[\"time_probe\"]))\n",
    "time_totals = np.array(time_totals)\n",
    "time_probes = np.array(time_probes)\n",
    "\n",
    "experiments[\"stripped\"] = (np.mean(time_totals), np.std(time_totals), np.mean(time_probes), np.std(time_probes))\n",
    "\n",
    "print(\"-------\")\n",
    "\n",
    "for expriment, (total_mean, total_std, probe_mean, probe_std) in experiments.items():\n",
    "    print(f\"{expriment} | Mean: {total_mean:.2f} s | Std: {total_std:.2f} s | Mean: {probe_mean:.2f} s | Std: {probe_std:.2f} s | {(probe_mean * 100 / total_mean):.2f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWMR Enabled: True\n",
      "File format version: ('v114', 'v114')\n",
      "(1460, 21, 721, 1440)\n",
      "float32\n",
      "(1, 21, 721, 1440)\n",
      "87212160\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1979.h5\"\n",
    "\n",
    "with h5py.File(file_path, \"r\") as f:\n",
    "    swmr_enabled = f.swmr_mode\n",
    "    libver = f.libver\n",
    "    print(f\"SWMR Enabled: {swmr_enabled}\")\n",
    "    print(f\"File format version: {libver}\")\n",
    "    print(f[\"fields\"].shape)\n",
    "    print(f[\"fields\"].dtype)\n",
    "    print(f[\"fields\"].chunks)\n",
    "    data = np.array(f[\"fields\"][0])\n",
    "    # print data size in MB\n",
    "    print(data.nbytes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1979.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1979.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1980.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1980.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1981.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1981.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1982.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1982.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1983.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1983.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1984.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1984.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1985.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1985.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1986.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1986.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1987.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1987.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1988.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1988.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1989.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1989.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1990.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1990.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1991.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1991.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1992.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1992.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1993.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1993.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1994.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1994.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1995.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1995.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1996.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1996.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1997.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1997.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1998.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1998.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1999.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1999.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2000.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2000.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2001.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2001.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2002.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2002.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2003.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2003.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2004.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2004.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2005.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2005.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2006.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2006.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2007.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2007.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2008.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2008.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2009.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2009.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2010.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2010.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2011.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2011.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2012.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2012.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2013.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2013.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2014.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2014.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/2015.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/2015.h5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train\"\n",
    "base_path_new = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr\"\n",
    "\n",
    "for path in sorted(Path(base_path).glob(\"????.h5\")):\n",
    "    old_file_path = str(path)\n",
    "    new_file_path = old_file_path.replace(base_path, base_path_new)\n",
    "\n",
    "    print(old_file_path, new_file_path)\n",
    "\n",
    "    optimized_chunk_size = (1, 21, 721, 1440)\n",
    "    with h5py.File(old_file_path, \"r\") as old_f, h5py.File(new_file_path, \"w\", libver=\"latest\") as new_f:\n",
    "\n",
    "        old_data = old_f[\"fields\"]\n",
    "        new_data = new_f.create_dataset(\n",
    "            \"fields\",\n",
    "            shape=old_data.shape,\n",
    "            dtype=old_data.dtype,\n",
    "            chunks=optimized_chunk_size,\n",
    "            track_times=False\n",
    "        )\n",
    "        new_data[:] = old_data[:]\n",
    "        new_f.swmr_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test/2016.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test_swmr/2016.h5\n",
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test/2017.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test_swmr/2017.h5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test\"\n",
    "base_path_new = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test_swmr\"\n",
    "\n",
    "for path in sorted(Path(base_path).glob(\"????.h5\")):\n",
    "    old_file_path = str(path)\n",
    "    new_file_path = old_file_path.replace(base_path, base_path_new)\n",
    "\n",
    "    print(old_file_path, new_file_path)\n",
    "\n",
    "    optimized_chunk_size = (1, 21, 721, 1440)\n",
    "    with h5py.File(old_file_path, \"r\") as old_f, h5py.File(new_file_path, \"w\", libver=\"latest\") as new_f:\n",
    "\n",
    "        old_data = old_f[\"fields\"]\n",
    "        new_data = new_f.create_dataset(\n",
    "            \"fields\",\n",
    "            shape=old_data.shape,\n",
    "            dtype=old_data.dtype,\n",
    "            chunks=optimized_chunk_size,\n",
    "            track_times=False\n",
    "        )\n",
    "        new_data[:] = old_data[:]\n",
    "        new_f.swmr_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/out_of_sample/2018.h5 /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/out_of_sample_swmr/2018.h5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/out_of_sample\"\n",
    "base_path_new = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/out_of_sample_swmr\"\n",
    "\n",
    "for path in sorted(Path(base_path).glob(\"????.h5\")):\n",
    "    old_file_path = str(path)\n",
    "    new_file_path = old_file_path.replace(base_path, base_path_new)\n",
    "\n",
    "    print(old_file_path, new_file_path)\n",
    "\n",
    "    optimized_chunk_size = (1, 21, 721, 1440)\n",
    "    with h5py.File(old_file_path, \"r\") as old_f, h5py.File(new_file_path, \"w\", libver=\"latest\") as new_f:\n",
    "\n",
    "        old_data = old_f[\"fields\"]\n",
    "        new_data = new_f.create_dataset(\n",
    "            \"fields\",\n",
    "            shape=old_data.shape,\n",
    "            dtype=old_data.dtype,\n",
    "            chunks=optimized_chunk_size,\n",
    "            track_times=False\n",
    "        )\n",
    "        new_data[:] = old_data[:]\n",
    "        new_f.swmr_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New SWMR-enabled HDF5 file created.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "old_file = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1979.h5\"\n",
    "new_file = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1979.h5\"\n",
    "\n",
    "with h5py.File(old_file, \"r\") as old_f, h5py.File(new_file, \"w\", libver=\"latest\") as new_f:\n",
    "    optimized_chunk_size = (1, 21, 721, 1440)\n",
    "\n",
    "    old_data = old_f[\"fields\"]\n",
    "    new_data = new_f.create_dataset(\n",
    "        \"fields\",\n",
    "        shape=old_data.shape,\n",
    "        dtype=old_data.dtype,\n",
    "        chunks=optimized_chunk_size,  # Optimized chunking\n",
    "        track_times=False  # Reduces metadata overhead\n",
    "    )\n",
    "\n",
    "    new_data[:] = old_data[:]\n",
    "\n",
    "    # Enable SWMR mode\n",
    "    new_f.swmr_mode = True\n",
    "\n",
    "print(\"New SWMR-enabled HDF5 file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWMR Enabled: True\n",
      "File format version: ('v110', 'v114')\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1979.h5\"\n",
    "\n",
    "with h5py.File(file_path, \"r\", swmr=True) as f:\n",
    "    swmr_enabled = f.swmr_mode\n",
    "    libver = f.libver\n",
    "    print(f\"SWMR Enabled: {swmr_enabled}\")\n",
    "    print(f\"File format version: {libver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader Time (Before SWMR): 33.88 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"HDF5_CHUNK_CACHE_SIZE\"] = str(100 * 1024 * 1024)  # 100MB\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file):\n",
    "        self.hdf5_file = hdf5_file\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.hdf5_file, \"r\") as f:\n",
    "            return f[\"fields\"][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.hdf5_file, \"r\") as f:\n",
    "            return len(f[\"fields\"])\n",
    "\n",
    "dataset = HDF5Dataset(\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1979.h5\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=8, prefetch_factor=4, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for batch in dataloader:\n",
    "    pass  # Simulating data loading\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"DataLoader Time (Before SWMR): {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader Time (After SWMR): 34.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "os.environ[\"HDF5_CHUNK_CACHE_SIZE\"] = str(100 * 1024 * 1024)  # 100MB\n",
    "os.environ[\"HDF5_SWMR_WRITE\"] = \"1\"\n",
    "os.environ[\"HDF5_SWMR_READ\"] = \"1\"\n",
    "\n",
    "class HDF5SWMRDataset(Dataset):\n",
    "    def __init__(self, hdf5_file):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\", libver=\"latest\", swmr=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.f[\"fields\"][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f[\"fields\"])\n",
    "\n",
    "dataset_swmr = HDF5SWMRDataset(\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr/1979.h5\")\n",
    "dataloader_swmr = DataLoader(dataset_swmr, batch_size=32, num_workers=8, prefetch_factor=4, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for batch in dataloader_swmr:\n",
    "    pass\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"DataLoader Time (After SWMR): {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lfs: command not found\n"
     ]
    }
   ],
   "source": [
    "!lfs getstripe /iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train/1979.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mProvided checkpoint directory /iopsstor/scratch/cscs/stefschu/DSM500/github/analysis/checkpoints does not exist, skipping load\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation datapipe of size 6\n"
     ]
    }
   ],
   "source": [
    "%run __common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[A] Allocated: 2702.00 MB | Cached: 2702.00 MB\n",
      "[B] Allocated: 2702.00 MB | Cached: 2702.00 MB\n",
      "[C] Allocated: 2702.00 MB | Cached: 2702.00 MB\n",
      "[D] Allocated: 960.00 MB | Cached: 3662.00 MB\n",
      "[E] Allocated: 960.00 MB | Cached: 3662.00 MB\n",
      "<class 'torch.Tensor'> torch.Size([1, 21, 721, 1440]) torch.float32\n",
      "<class 'torch.Tensor'> torch.Size([1, 10, 21, 721, 1440]) torch.float32\n",
      "<class 'torch.Tensor'> torch.Size([1, 11, 1, 721, 1440]) torch.float32\n",
      "217.8314208984375\n",
      "0.8827576483449628 0.8889545674599225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1022: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from modulus.datapipes.climate.era5_hdf5 import ERA5HDF5Datapipe\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def list_cuda_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                print(type(obj), obj.size(), obj.dtype)\n",
    "        except:\n",
    "            pass  # Skip any errors\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    cached = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"[{tag}] Allocated: {allocated:.2f} MB | Cached: {cached:.2f} MB\")\n",
    "\n",
    "def tensor_memory(tensor):\n",
    "    return tensor.element_size() * tensor.numel() / 1024**2  # Convert to MB\n",
    "\n",
    "print_memory_usage(\"A\")\n",
    "datapipe = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train\",\n",
    "    stats_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    channels=list(range(21)),\n",
    "    latlon_resolution=[721, 1440],\n",
    "    interpolation_type=None,\n",
    "    num_samples_per_year=1408,\n",
    "    num_steps=10,\n",
    "    num_history=0,\n",
    "    use_cos_zenith=True,\n",
    "    use_time_of_year_index=True,\n",
    "    cos_zenith_args={\n",
    "        \"dt\": 6.0,\n",
    "        \"start_year\": 1979,\n",
    "    },\n",
    "    batch_size=1,\n",
    "    num_workers=8,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    "    process_rank=0,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "print_memory_usage(\"B\")\n",
    "#train_loader = data.DataLoader(train, batch_size=1, shuffle=True)\n",
    "\n",
    "print_memory_usage(\"C\")\n",
    "import time\n",
    "times = []\n",
    "t0 = time.time()\n",
    "for i, sample in enumerate(datapipe):\n",
    "    times.append(time.time() - t0)\n",
    "    t0 = time.time()\n",
    "    #print(i, sample.shape, sample.dtype)\n",
    "    #print(sample.mean(dim=(0, 1, 3, 4)), sample.std(dim=(0, 1, 3, 4)))\n",
    "    #print()\n",
    "    if i == 30:\n",
    "        break\n",
    "\n",
    "print_memory_usage(\"D\")\n",
    "print_memory_usage(\"E\")\n",
    "\n",
    "list_cuda_tensors()\n",
    "print(tensor_memory(train.static_data))\n",
    "\n",
    "import numpy as np\n",
    "times = np.array(times)\n",
    "\n",
    "print(np.mean(times), np.std(times))\n",
    "# means = sample[0,:].mean(dim=(0, 2, 3))\n",
    "# stds = sample[0,:].std(dim=(0, 2, 3))\n",
    "# for m, s in zip(means, stds):\n",
    "#     print(f\"{m.item():.4f}, \\t, {s.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m), ts[:, \u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msin(progress_in_year)\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mnum_samples_per_year_train)), ts[:, \u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos(progress_in_year)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime step in the year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = list(range(sample.shape[1]))\n",
    "# y1 = sample[0, :, 27, 0, 0].cpu().to(torch.float32).numpy()\n",
    "# y2 = sample[0, :, 28, 0, 0].cpu().to(torch.float32).numpy()\n",
    "\n",
    "# x, y1, y2\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(x, y1, label=f'sin(progress_in_year)')\n",
    "# plt.plot(x, y2, label=f'cos(progress_in_year)')\n",
    "# plt.xlabel(\"Time step in the year\")\n",
    "# plt.ylabel(\"Value\")\n",
    "# plt.title(\"Time of year\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# x = list(range(9))\n",
    "# y1 = sample[0, :9, 29, 0, 0].cpu().to(torch.float32).numpy()\n",
    "# y2 = sample[0, :9, 30, 0, 0].cpu().to(torch.float32).numpy()\n",
    "\n",
    "# x, y1, y2\n",
    "\n",
    "# plt.figure(figsize=(10, 2))\n",
    "# plt.plot(x, y1, label=f'sin(progress_in_day)')\n",
    "# plt.plot(x, y2, label=f'cos(progress_in_day)')\n",
    "# plt.xlabel(\"Time step in the year\")\n",
    "# plt.ylabel(\"Value\")\n",
    "# plt.title(\"Time of year\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to init ERA5HDF5Datapipe: 0.00s\n",
      "ERA5HDF5Datapipe.__init__ 218650 709.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from modulus.datapipes.climate.era5_hdf5_new_dali import ERA5HDF5Datapipe\n",
    "\n",
    "datapipe = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr\",\n",
    "    statistics_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    static_data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\",\n",
    "    num_steps_ahead=10,\n",
    "    device=\"cuda:0\",\n",
    "    dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datapipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatapipe\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sample), sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datapipe' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "for sample in datapipe:\n",
    "    print(len(sample), sample[0][\"input\"].shape, sample[0][\"output\"].shape, f\"{(time.time() - t0):.2f} s\")\n",
    "    t0 = time.time()\n",
    "    #print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/modulus/distributed/manager.py:329: UserWarning: Distributed manager is already intialized\n",
      "  warn(\"Distributed manager is already intialized\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXX\n",
      "tensor([[1.3172e+09]], dtype=torch.float64) 1317160800\n",
      "2011-09-28 00:00:00 torch.Size([1, 1, 31, 721, 1440]) torch.Size([1, 1, 31, 721, 1440]) 4.23 s\n",
      "tensor([[8.5639e+08]], dtype=torch.float64) 856393200\n",
      "1997-02-20 00:00:00 torch.Size([1, 1, 31, 721, 1440]) torch.Size([1, 1, 31, 721, 1440]) 0.14 s\n",
      "tensor([[7.0092e+08]], dtype=torch.float64) 700916400\n",
      "1992-03-18 12:00:00 torch.Size([1, 1, 31, 721, 1440]) torch.Size([1, 1, 31, 721, 1440]) 0.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 721, 1440])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# distributed manager initialization (by mocking a simple slurm job)\n",
    "from modulus.distributed import DistributedManager\n",
    "import os\n",
    "import random\n",
    "os.environ[\"MODULUS_DISTRIBUTED_INITIALIZATION_METHOD\"] = \"ENV\"\n",
    "os.environ[\"MASTER_PORT\"] = str(random.randint(10000, 20000)) # <--- Several notebooks can be running at the same time, none of which is running anything distributed\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "DistributedManager.initialize()\n",
    "dist = DistributedManager()\n",
    "\n",
    "import torch\n",
    "from modulus.datapipes.climate.era5_hdf5_new_dali_mem import ERA5HDF5Datapipe\n",
    "\n",
    "datapipe = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/train_swmr\",\n",
    "    statistics_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    static_data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\",\n",
    "    dist=dist,\n",
    ")\n",
    "\n",
    "print(\"XXX\", flush=True)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "t0 = time.time()\n",
    "for i, sample in enumerate(datapipe):\n",
    "    print(g(sample[0][\"input_timestamps\"]), sample[0][\"input\"].shape, sample[0][\"output\"].shape, f\"{(time.time() - t0):.2f} s\")\n",
    "    t0 = time.time()\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "sample = sample[0]\n",
    "xsample = torch.concatenate([sample[\"input\"], sample[\"output\"]], dim=1)[0]\n",
    "xsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700916400.0, 700938000.0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_timestamps\"].item(), sample[\"output_timestamps\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 4 5 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "print(arr)  # The order is changed in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/iopsstor/scratch/cscs/stefschu/DSM500/github/venv_arm64/lib/python3.10/site-packages',\n",
       " '__editable__.nvidia_modulus-0.10.0a0.finder.__path_hook__',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages/nvfuser-0.2.10a0+f669fcf-py3.10-linux-aarch64.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/lightning_thunder-0.2.0.dev0-py3.10.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/dill-0.3.9-py3.10.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/opt_einsum-3.4.0-py3.10.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/igraph-0.11.6-py3.10-linux-aarch64.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/lightning_utilities-0.11.7-py3.10.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/looseversion-1.3.0-py3.10.egg',\n",
       " '/usr/local/lib/python3.10/dist-packages/texttable-1.7.0-py3.10.egg',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/iopsstor/scratch/cscs/stefschu/DSM500/github/venv_arm64/lib/python3.10/site-packages/setuptools/_vendor']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 31, 721, 1440), (1, 21, 721, 1440))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.ones((1, 3, 31, 721, 1440))\n",
    "prediction = np.zeros((1, 21, 721, 1440))\n",
    "\n",
    "output.shape, prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 21, 721, 1440)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = prediction.reshape(1, 1, 21, 721, 1440)\n",
    "print(temp.shape)\n",
    "\n",
    "np.all(temp[0] == prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 10, 721, 1440)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, [1], 21:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 31, 721, 1440)\n",
      "(0, 0.0)\n",
      "(1, 0.0)\n",
      "(2, 0.0)\n",
      "(3, 0.0)\n",
      "(4, 0.0)\n",
      "(5, 0.0)\n",
      "(6, 0.0)\n",
      "(7, 0.0)\n",
      "(8, 0.0)\n",
      "(9, 0.0)\n",
      "(10, 0.0)\n",
      "(11, 0.0)\n",
      "(12, 0.0)\n",
      "(13, 0.0)\n",
      "(14, 0.0)\n",
      "(15, 0.0)\n",
      "(16, 0.0)\n",
      "(17, 0.0)\n",
      "(18, 0.0)\n",
      "(19, 0.0)\n",
      "(20, 0.0)\n",
      "(21, 1038240.0)\n",
      "(22, 1038240.0)\n",
      "(23, 1038240.0)\n",
      "(24, 1038240.0)\n",
      "(25, 1038240.0)\n",
      "(26, 1038240.0)\n",
      "(27, 1038240.0)\n",
      "(28, 1038240.0)\n",
      "(29, 1038240.0)\n",
      "(30, 1038240.0)\n"
     ]
    }
   ],
   "source": [
    "a = np.concatenate([temp, output[:, [1], 21:]], axis=2)\n",
    "print(a.shape)\n",
    "for i in enumerate(a.sum(axis=(0, 1, 3, 4))):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study why validation takes twice as long with the new dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "with hydra.initialize(config_path=\".\", version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=\"config\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "from modulus.distributed import DistributedManager\n",
    "os.environ[\"MODULUS_DISTRIBUTED_INITIALIZATION_METHOD\"] = \"ENV\"\n",
    "os.environ[\"MASTER_PORT\"] = str(random.randint(10000, 20000)) # <--- Several notebooks can be running at the same time, none of which is running anything distributed\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "DistributedManager.initialize()\n",
    "dist = DistributedManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mProvided checkpoint directory /iopsstor/scratch/cscs/stefschu/DSM500/github/analysis/checkpoints does not exist, skipping load\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusedAdam not available, using AdamW\n",
      "Loaded validation datapipe of size 2912\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/iopsstor/scratch/cscs/stefschu/DSM500/github/modulus-a5275d8/examples/weather/graphcast')\n",
    "\n",
    "from modulus.launch.logging import (\n",
    "    PythonLogger,\n",
    "    RankZeroLoggingWrapper\n",
    ")\n",
    "from omegaconf import DictConfig\n",
    "logger = PythonLogger(\"main\")  # General python logger\n",
    "rank_zero_logger = RankZeroLoggingWrapper(logger, dist)  # Rank 0 logger\n",
    "rank_zero_logger.file_logging()\n",
    "\n",
    "# Instantiate dummy trainer\n",
    "from train_graphcast import GraphCastTrainer\n",
    "trainer = GraphCastTrainer(cfg, dist, rank_zero_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline length: 2912\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "0: Data loading + prep: 4.47 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "1: Data loading + prep: 0.06 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "2: Data loading + prep: 0.18 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "3: Data loading + prep: 0.27 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "4: Data loading + prep: 0.25 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "5: Data loading + prep: 0.18 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "6: Data loading + prep: 0.18 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "7: Data loading + prep: 0.18 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "8: Data loading + prep: 0.24 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "9: Data loading + prep: 0.13 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "10: Data loading + prep: 0.23 s, going to loop 8 times\n",
      "torch.Size([1, 1, 31, 721, 1440]) torch.float32 torch.Size([1, 8, 31, 721, 1440]) torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.1059, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modulus.datapipes.climate.era5_hdf5_new import ERA5HDF5Datapipe\n",
    "\n",
    "val_datapipe = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test_swmr\",\n",
    "    statistics_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    static_data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\",\n",
    "    num_steps=cfg.num_val_steps,\n",
    "    dist=dist,\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "mse = 0\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Pipeline length: {len(val_datapipe)}\")\n",
    "for i, data in enumerate(val_datapipe):\n",
    "    print(data[0][\"input\"].shape, data[0][\"input\"].dtype, data[0][\"output\"].shape, data[0][\"output\"].dtype)\n",
    "    \n",
    "    if i > 10: # cfg.num_val_spy:\n",
    "        break\n",
    "\n",
    "    # Prepare the input & output\n",
    "    input = data[0][\"input\"].to(dtype=torch.bfloat16)\n",
    "    output = data[0][\"output\"]\n",
    "\n",
    "    predictions = (\n",
    "        torch.empty(output.shape)\n",
    "        .to(dtype=torch.bfloat16)\n",
    "        .to(device=dist.device)\n",
    "    )\n",
    "    print(f\"{i}: Data loading + prep: {time.time() - t0:.2f} s, going to loop {output.size(dim=1)} times\")\n",
    "    #t0 = time.time()\n",
    "    for t in range(output.size(dim=1)):\n",
    "        #_pred = input[0, :, :21]\n",
    "        #_pred = trainer.model(input)\n",
    "\n",
    "        #_pred = _pred.reshape(1, 1, 21, 721, 1440)\n",
    "        #_pred = torch.cat([_pred, output[:, [t], 21:]], axis=2)\n",
    "\n",
    "        #predictions[0, t] = _pred\n",
    "        #input = _pred\n",
    "        time.sleep(.5)\n",
    "    t0 = time.time()\n",
    "    mse += torch.mean(torch.pow(predictions - output, 2))\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    #del input, _pred\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del predictions, output\n",
    "\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline length: 10\n",
      "Time to extract data: 0.24067306518554688\n",
      "Time to extract data: 0.2447528839111328\n",
      "Time to extract data: 0.2325267791748047\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "0: Data loading + prep: 4.16 s, going to loop 8 times\n",
      "Time to extract data: 0.24914026260375977\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "1: Data loading + prep: 0.21 s, going to loop 8 times\n",
      "Time to extract data: 0.2470099925994873\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "2: Data loading + prep: 0.26 s, going to loop 8 times\n",
      "Time to extract data: 0.46944570541381836\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "3: Data loading + prep: 0.24 s, going to loop 8 times\n",
      "Time to extract data: 0.23991680145263672\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "4: Data loading + prep: 0.17 s, going to loop 8 times\n",
      "Time to extract data: 0.2537868022918701\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "5: Data loading + prep: 0.18 s, going to loop 8 times\n",
      "Time to extract data: 0.24264788627624512\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "6: Data loading + prep: 0.14 s, going to loop 8 times\n",
      "Time to extract data: 0.23552989959716797\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "7: Data loading + prep: 0.21 s, going to loop 8 times\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "8: Data loading + prep: 0.15 s, going to loop 8 times\n",
      "torch.Size([1, 21, 721, 1440]) torch.float32 torch.Size([1, 8, 21, 721, 1440]) torch.float32\n",
      "9: Data loading + prep: 0.29 s, going to loop 8 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-196:\n",
      "Process SpawnProcess-199:\n",
      "Process SpawnProcess-195:\n",
      "Process SpawnProcess-194:\n",
      "Process SpawnProcess-197:\n",
      "Process SpawnProcess-193:\n",
      "Process SpawnProcess-200:\n",
      "Process SpawnProcess-198:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 462, in worker\n",
      "    scheduled, shm_chunk = worker_context.get_task()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 434, in get_task\n",
      "    scheduled_meta = self.task_receiver.get_task()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/worker.py\", line 132, in get_task\n",
      "    recv = self.queue.get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 171, in get\n",
      "    waited = self._wait_for_samples()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvidia/dali/_multiproc/shared_queue.py\", line 99, in _wait_for_samples\n",
      "    self.cv_not_empty.wait()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
      "    return self._wait_semaphore.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from modulus.datapipes.climate import ERA5HDF5Datapipe\n",
    "from train_utils import prepare_input\n",
    "from modulus.utils.graphcast.data_utils import StaticData\n",
    "\n",
    "latitudes = trainer.model.latitudes\n",
    "longitudes = trainer.model.longitudes\n",
    "static_data = StaticData(\n",
    "    \"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/static\", latitudes, longitudes\n",
    ").get()\n",
    "static_data = static_data.to(device=dist.device)\n",
    "\n",
    "interpolation_type = None\n",
    "cos_zenith_args = {\n",
    "    \"dt\": 6.0,\n",
    "    \"start_year\": 2017,\n",
    "}\n",
    "val_datapipe = ERA5HDF5Datapipe(\n",
    "    data_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/test_swmr\",\n",
    "    stats_dir=\"/iopsstor/scratch/cscs/stefschu/DSM500/data/FCN_ERA5_data_v0/stats\",\n",
    "    channels=[i for i in range(cfg.num_channels_climate)],\n",
    "    latlon_resolution=cfg.latlon_res,\n",
    "    interpolation_type=interpolation_type,\n",
    "    num_steps=cfg.num_val_steps,\n",
    "    num_history=cfg.num_history,\n",
    "    use_cos_zenith=cfg.use_cos_zenith,\n",
    "    use_time_of_year_index=cfg.use_time_of_year_index,\n",
    "    cos_zenith_args=cos_zenith_args,\n",
    "    batch_size=1,\n",
    "    num_samples_per_year=5,#cfg.num_val_spy,\n",
    "    shuffle=False,\n",
    "    device=dist.device,\n",
    "    process_rank=dist.rank,\n",
    "    world_size=dist.world_size,\n",
    "    num_workers=cfg.num_workers,\n",
    ")\n",
    "\n",
    "num_history = cfg.num_history\n",
    "stride = cfg.stride\n",
    "dt = cfg.dt\n",
    "num_samples_per_year_train = cfg.num_samples_per_year_train\n",
    "\n",
    "import time\n",
    "\n",
    "loss_epoch = 0\n",
    "prepare_input_vars = {\n",
    "    \"num_history\": num_history,\n",
    "    \"static_data\": static_data,\n",
    "    \"stride\": stride,\n",
    "    \"dt\": dt,\n",
    "    \"num_samples_per_year\": num_samples_per_year_train,\n",
    "    \"device\": dist.device,\n",
    "}\n",
    "t0 = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Pipeline length: {len(val_datapipe)}\")\n",
    "for i, data in enumerate(val_datapipe):\n",
    "    print(data[0][\"invar\"].shape, data[0][\"invar\"].dtype, data[0][\"outvar\"].shape, data[0][\"outvar\"].dtype)\n",
    "    invar = data[0][\"invar\"]\n",
    "    outvar = data[0][\"outvar\"][0]\n",
    "    try:\n",
    "        cos_zenith = data[0][\"cos_zenith\"]\n",
    "    except KeyError:\n",
    "        cos_zenith = None\n",
    "    try:\n",
    "        time_idx = data[0][\"time_of_year_idx\"].item()\n",
    "    except KeyError:\n",
    "        time_idx = None\n",
    "    invar_cat = prepare_input(\n",
    "        invar=invar,\n",
    "        cos_zenith=cos_zenith,\n",
    "        time_idx=time_idx,\n",
    "        **prepare_input_vars,\n",
    "        step=1,\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    invar_cat = invar_cat.to(dtype=torch.bfloat16)\n",
    "\n",
    "    pred = (\n",
    "        torch.empty(outvar.shape)\n",
    "        .to(dtype=torch.bfloat16)\n",
    "        .to(device=dist.device)\n",
    "    )\n",
    "    print(f\"{i}: Data loading + prep: {time.time() - t0:.2f} s, going to loop {outvar.shape[0]} times\")\n",
    "    \n",
    "    for t in range(outvar.shape[0]):\n",
    "        time.sleep(.5)\n",
    "    t0 = time.time()\n",
    "    loss_epoch += torch.mean(torch.pow(pred - outvar, 2))\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    #del invar, outpred\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 1, 31, 721, 1440]) torch.float32\n",
      "<class 'torch.Tensor'> torch.Size([1, 8, 31, 721, 1440]) torch.float32\n",
      "<class 'torch.Tensor'> torch.Size([1, 8]) torch.float64\n",
      "<class 'torch.Tensor'> torch.Size([1, 1]) torch.float64\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 4]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 31]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 3]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 4]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 4]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([0]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816, 4]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([3114720, 4]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 3]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([327660, 4]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([21, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([21]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([721, 1440]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 21, 1, 1]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 21, 1, 1]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([327660, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 1, 31, 721, 1440]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 31, 721, 1440]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 31]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 1, 31, 721, 1440]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 31]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([327660, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1038240, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([40962, 512]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1618816]) torch.int64\n",
      "<class 'torch.Tensor'> torch.Size([1618816]) torch.int64\n",
      "<class 'torch.Tensor'> torch.Size([]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 8, 31, 721, 1440]) torch.bfloat16\n",
      "<class 'torch.Tensor'> torch.Size([1, 8, 31, 721, 1440]) torch.bfloat16\n",
      "[] Allocated: 10599.42 MB | Cached: 15184.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def list_cuda_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                print(type(obj), obj.size(), obj.dtype)\n",
    "        except:\n",
    "            pass  # Skip any errors\n",
    "\n",
    "def print_memory_usage(tag=\"\"):\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    cached = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"[{tag}] Allocated: {allocated:.2f} MB | Cached: {cached:.2f} MB\")\n",
    "\n",
    "list_cuda_tensors()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000,\n",
       " 1001,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1011,\n",
       " 1012,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(range(1000, 1016), dtype=np.float32)\n",
    "a.astype(np.int32).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.53125, dtype('uint16'), 9999)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.8828125, dtype('uint16'), 9999)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(10000, dtype=np.uint16)\n",
    "indices.nbytes / 1024, indices.dtype, indices.max()\n",
    "np.random.default_rng(seed=4).shuffle(indices)\n",
    "indices = np.array_split(indices, 4)\n",
    "indices = indices[2]\n",
    "indices.nbytes / 1024, indices.dtype, indices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.indices = np.arange(self.num_samples, dtype=np.uint16)\n",
    "        if shuffled:\n",
    "            seed = sample_info.epoch_idx + int(os.getenv(\"SLURM_JOB_ID\", 0))\n",
    "            np.random.default_rng(seed=seed).shuffle(self.indices)\n",
    "        self.indices = np.array_split(self.indices, self.dist.world_size)\n",
    "        self.indices = self.indices[self.dist.rank]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_arm64",
   "language": "python",
   "name": "venv_arm64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
